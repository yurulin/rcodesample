---
title       : Class03
subtitle    : binary classification
author      : 
job         : 
framework   : shower        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
--- #toc

## Class03
* [Set up](#set-up)
* [Split data into two classes](#two-class)
* [Logistic Regression](#logistic-regression)
* [kNN](#knn)

--- #set-up .modal 

## Install R packages
```{r eval=FALSE}
## this tutorial uses the following packages
```

--- #two-class .scode-nowrap .compact 

## Split data into two classes
```{r, fig.height=4, fig.width=7}
library(ggplot2) ## use ggplot for plotting
data.url = 'http://www.yurulin.com/class/spring2014_datamining/data/ml_hackers/'
data.file = file.path(data.url , '01_heights_weights_genders.csv')
heights.weights <- read.csv(data.file , header = TRUE , sep = ',')
# visualized their gender as the color of each point
ggplot(heights.weights, aes(x = Weight , y = Height , color = Gender)) +
  geom_point()
```

--- .scode-nowrap .compact 
## Split data into two classes

```{r, fig.height=4, fig.width=4, dependson=c(-1)}
heights.weights = transform(heights.weights , Male = ifelse(Gender == 'Male', 1, 0))
heights.weights [1:3,]
```

--- #logistic-regression .model
## Logistic Regression

Example 1: Death penelty data
-----------------------------

   * objective: to identify whether the death penalty more likely if the victim is white, or the crime is more aggravating
   * response variable: getting death panelty or not
   * explanatory variables: severity of the crime, race of victim (black: 0; white: 1), etc

--- .scode-nowrap .compact 
## Logistic Regression
```{r}
## analyzing individual observations
data.url = 'http://www.yurulin.com/class/spring2014_datamining/data/data_text'
dpen = read.csv(sprintf("%s/DeathPenalty.csv",data.url))
dpen[1:4,]
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
m1=glm(Death~VRace+Agg,family=binomial(link='logit'),data=dpen)
m1
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
summary(m1)
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
## calculating logits
exp(m1$coef[2])
exp(m1$coef[3])
```

--- .ssscode-nowrap .compact
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## plotting probability of getting death penalty as a function of aggravation
## separately for black (in black) and white (in red) victim
n.dots = 501 ## number of dots in the curves (the nubmer is set to make the curves smooth)
x.ag=dim(n.dots) ## x-value is the aggravating level (severity of the crime)
y.black=dim(n.dots) ## y-value for the black victim curve
y.white=dim(n.dots) ## y-value for the white victim curve
for (i in 1:n.dots) {
  x.ag[i]=(99+i)/100
  is.white = 0
  y.black[i]=exp(m1$coef[1]+m1$coef[2]*is.white+m1$coef[3]*x.ag[i])/(1+exp(m1$coef[1]+m1$coef[2]*is.white+m1$coef[3]*x.ag[i]))
  is.white = 1
  y.white[i]=exp(m1$coef[1]+m1$coef[2]*is.white+m1$coef[3]*x.ag[i])/(1+exp(m1$coef[1]+m1$coef[2]*is.white+m1$coef[3]*x.ag[i]))
}
plot(y.black~x.ag,type="l",col="black",ylab="Prob[Death]",xlab="Aggravation",ylim=c(0,1),main="red: white victim; black: black victim")
points(y.white~x.ag,type="l",col="red")
```

--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
## another format for logistic regression:
## summarized data
dpenother <- read.csv(sprintf("%s/DeathPenaltyOther.csv",data.url))
dpenother
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
m1=glm(Death~VRace+Agg,family=binomial,weights=Freq,data=dpenother)
m1
```

--- .ssscode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
summary(m1)
exp(m1$coef[2])
exp(m1$coef[3])
```


--- .model
## Logistic Regression

Example: Delayed Flights
------------------------

   * response variable: whether or not a flight has been delayed by more than 15 min (coded as 0 for no delay, and 1 for delay)
   * explanatory variables: different arrival airports, different departure airports, eight carriers, different hours of departure (6am to 10 pm), weather conditions (0 = good/1 = bad), day of week (1 for Sunday and Monday; and 0 for all other days)
   * objective: to identify flights that are likely to be delayed (a binary classification problem)


--- .scode-nowrap .compact 
## Logistic Regression
```{r}
library(car) ## needed to recode variables
set.seed(1) ## reset random seed for reproducibility

## read and print the data
data.url = 'http://www.yurulin.com/class/spring2014_datamining/data/data_text'
del <- read.csv(sprintf("%s/FlightDelays.csv",data.url))
del[1:3,]
```

--- .scode-nowrap .compact 
## Logistic Regression
* Convert data into desirable format: record and clean variables

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## define hours of departure
del$sched=factor(floor(del$schedtime/100))
del$delay=recode(del$delay,"'delayed'=1;else=0")
del$delay=as.numeric(levels(del$delay)[del$delay])
table(del$delay)
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
## Delay: 1=Monday; 2=Tuesday; 3=Wednesday; 4=Thursday; 5=Friday; 6=Saturday; 7=Sunday
## 7=Sunday and 1=Monday coded as 1
del$dayweek=recode(del$dayweek,"c(1,7)=1;else=0")
table(del$dayweek)

## omit unused variables
del=del[,c(-1,-3,-5,-6,-7,-11,-12)]
del[1:3,]
```


--- .scode-nowrap .compact 
## Logistic Regression
```{r, dependson=c(-1), fig.height=4, fig.width=4}
## estimation of the logistic regression model
## explanatory variables: carrier, destination, origin, weather, day of week 
## (weekday/weekend), scheduled hour of departure
## create design matrix; indicators for categorical variables (factors)
Xdel = model.matrix(delay~.,data=del)[,-1] 
Xdel[1:3,]
```


--- .scode-nowrap .compact 
## Logistic Regression
* Split the data into 60% training and 40% testing

```{r, dependson=c(-1), fig.height=4, fig.width=4}
n.total=length(del$delay)
n.total
n.train=floor(n.total*(0.6))
n.train
n.test=n.total-n.train
n.test
```

--- .sscode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
train=sample(1:n.total,n.train) ## (randomly) sample indices for training set

xtrain = Xdel[train,]
xtest = Xdel[-train,]
ytrain = del$delay[train]
ytest = del$delay[-train]
m1 = glm(delay~.,family=binomial,data=data.frame(delay=ytrain,xtrain))
summary(m1)
```

--- .ssscode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4, echo=FALSE}
train=sample(1:n.total,n.train) ## (randomly) sample indices for training set

xtrain = Xdel[train,]
xtest = Xdel[-train,]
ytrain = del$delay[train]
ytest = del$delay[-train]
m1 = glm(delay~.,family=binomial,data=data.frame(delay=ytrain,xtrain))
summary(m1)
```

--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## prediction: predicted default probabilities for cases in test set
ptest = predict(m1,newdata=data.frame(xtest),type="response")
## the default predictions are of log-odds (probabilities on logit scale); 
## type = "response" gives the predicted probabilities
data.frame(ytest,ptest)[1:10,] ## look at the actual value vs. predicted value
```



--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## measuring errors: coding as 1 if probability 0.5 or larger
btest=floor(ptest+0.5)  ## use floor function to clamp the value to 0 or 1
conf.matrix = table(ytest,btest)
conf.matrix

error=(conf.matrix[1,2]+conf.matrix[2,1])/n.test
error
```



--- .sscode-nowrap .compact 
## Logistic Regression

Example: the delayed flights (Lift chart)
-----------------------------------------

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## order cases in test set according to their success prob
## actual outcome shown next to it
df=cbind(ptest,ytest)
df[1:20,]
```


--- .sscode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
rank.df=as.data.frame(df[order(ptest,decreasing=TRUE),])
colnames(rank.df) = c('predicted','actual')
rank.df[1:20,]
```



--- .sscode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## overall success (delay) prob in the evaluation data set
baserate=mean(ytest)
baserate

## calculating the lift
## cumulative 1's sorted by predicted values
## cumulative 1's using the average success prob from evaluation set
ax=dim(n.test)
ay.base=dim(n.test)
ay.pred=dim(n.test)
ax[1]=1
ay.base[1]=baserate
ay.pred[1]=rank.df$actual[1]
for (i in 2:n.test) {
  ax[i]=i
  ay.base[i]=baserate*i ## uniformly increase with rate xbar
  ay.pred[i]=ay.pred[i-1]+rank.df$actual[i]
}

df=cbind(rank.df,ay.pred,ay.base)
df[1:20,]
```



--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4,fig.align='center'}
plot(ax,ay.pred,xlab="number of cases",ylab="number of successes",
main="Lift: Cum successes sorted by\n pred val/success prob")
points(ax,ay.base,type="l")
```


--- .sscode-nowrap .compact 
## Logistic Regression

Example: the delayed flights (ROC)
----------------------------------

```{r, dependson=c(-1), fig.height=4, fig.width=4}
## we use probability cutoff 1/2
## coding as 1 if probability 1/2 or larger
cut=1/2
gg1=floor(ptest+(1-cut))
conf.matrix=table(ytest,btest)
conf.matrix

truepos <- ytest==1 & ptest>=cut 
trueneg <- ytest==0 & ptest<cut
# Sensitivity (predict default when it does happen)
sum(truepos)/sum(ytest==1) 

# Specificity (predict no default when it does not happen)
sum(trueneg)/sum(ytest==0) 
```

--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE, message=FALSE}
## using the ROCR package to graph the ROC curves 
library(ROCR)

## input is a data frame consisting of two columns
## predictions in first column and actual outcomes in the second 

## ROC for hold-out period
data=data.frame(predictions=ptest,labels=ytest)
data[1:10,]
```

--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
## pred: function to create prediction objects
pred <- prediction(data$predictions,data$labels)
str(pred)
```


--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
## perf: creates the input to be plotted
## sensitivity (TPR) and one minus specificity (FPR)
perf <- performance(pred, "sens", "fpr")
str(perf)
```


--- .scode-nowrap .compact 
## Logistic Regression

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
plot(perf)
```

--- #knn .scode-nowrap .compact 
## kNN

Example: Forensic Glass
-----------------------------

data set of 214 glass shards of six possible glass types
objective: to determine why types of glass based on characteristics including the refractive index (RI) and the percentages of Na, Mg, Al, Si, K, Ca, Ba, and Fe
6 groups of glass shards:
  
   WinF: float glass window
   WinNF: nonfloat window
   Veh: vehicle window
   Con: container (bottles)
   Tabl: tableware
   Head: vehicle headlamp
   
```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
suppressMessages( library(textir) ) ## help standardize the data
suppressMessages( library(MASS) )   ## a library of example datasets

data(fgl)     ## loads the data into R; see help(fgl)
fgl[1:3,]
```

--- .scode-nowrap .compact 
## kNN

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
## use nt=200 training cases to find the nearest neighbors for 
## the remaining 14 cases. These 14 cases become the evaluation 
## (test, hold-out) cases

n=length(fgl$type)
nt=200
set.seed(1) ## to make the calculations reproducible in repeated runs
train <- sample(1:n,nt)

## Standardization of the data is preferable, especially if 
## units of the features are quite different
## could do this from scratch by calculating the mean and 
## standard deviation of each feature, and use those to 
## standardize.
## Even simpler, use the normalize function in the R-package textir; 
## it converts data frame columns to mean-zero sd-one

x <- normalize(fgl[,c(4,1)]) ## you can also use scale()
x <- normalize(fgl[,c(1:9)]) ## using all nine dimensions (RI plus 8 chemical concentrations)
x[1:3,]
```


--- .scode-nowrap .compact 
## kNN

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
library(class)  
nearest1 <- knn(train=x[train,],test=x[-train,],cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,],test=x[-train,],cl=fgl$type[train],k=5)
data.frame(fgl$type[-train],nearest1,nearest5)
```



--- .scode-nowrap .compact 
## kNN

```{r, dependson=c(-1), fig.height=4, fig.width=4, warning=FALSE}
## calculate the proportion of correct classifications on this one 
## training set

pcorrn1=100*sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn5=100*sum(fgl$type[-train]==nearest5)/(n-nt)
pcorrn1
pcorrn5

## Note: Different runs may give you slightly different results as ties 
## are broken at random
```