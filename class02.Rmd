---
title       : Class02
subtitle    : regression examples
author      : 
job         : 
framework   : shower        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
--- #toc

## Class02
* [Set up](#set-up)
* [Simple Regression](#simple-regression)
* [Multiple linear regression](#multiple-linear-regression)
* [Variable selection](#variable-selection)
* [Model performance](#model-performance)
* [Polynomial regression](#polynomial-regression)
* [Non-linear Data](#non-linear)
* [Cross-validation](#cross-validation)
* [Regularization](#regularization)
* [Local Polynomial Regression](#local-poly)
* [LASSO](#lasso)

--- #set-up .modal 

## Install R packages
```{r eval=FALSE}
## this tutorial uses the following packages
install.packages('nutshell') 
install.packages('locfit')
install.packages('lars')
```

--- #simple-regression .scode .compact

## Simple Regression

The examples are taken from [R in Action](http://www.manning.com/kabacoff/)
$$\hat{y}=\alpha +\beta x$$

```{r}
## we'll use an exmaple dataset 'women' in the 'car' package
library(car) ## load the package
women
```

--- .modal

## Simple Regression

```{r}
## data summary
summary(women)
```

--- .modal

## Simple Regression

```{r, fig.height=4, fig.width=4}
## plot the data
plot(women$height, women$weight)
```

--- .modal

## Simple Regression

```{r, fig.height=4, fig.width=4}
library(ggplot2) ## load the plotting package
theme_set(theme_bw()) ## set default theme with a white background
ggplot(data=women, aes(x=height,y=weight)) + geom_point() 
```

--- .modal

## Simple Regression

```{r, fig.height=4, fig.width=4}
## plot with regression line
ggplot(women, aes(x = height, y = weight)) + geom_point() + 
  geom_smooth(method=lm, # add linear regression line
              se=FALSE) # (by default includes 95% confidence region)  
```

--- .scode

## Simple Regression

```{r}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### What does the result mean?  

*** =left
$$ \hat{weight}=-87.52+3.45\times height $$

* There's an expected increase of 3.45 pounds of weight for every 1 inch increase in height. 
* The intercept is merely an adjustment constant.  


*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(1) Is the model statistically significant?**

* Check the **F statistic** at the bottom of the summary.
* The F statistic tells you whether the model is insignificant or significant. Big p-value indicates a high likelihood of insignificance. (Further reading: F-statistic [1](http://www.statisticshowto.com/f-statistic/), [2](https://onlinecourses.science.psu.edu/stat501/node/295).)

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(2) Are the coefficients significant?**

   * Check the coefficient's **t statistics** and **p-values** in the summary, or check their confidence intervals.
   * If a variable's coefficient is zero then the variable is worthless; it adds nothing to the model. 

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(2) Are the coefficients significant?**

   * The p-value is a probability that the coefficient is not significant. Big is bad because it indicates a high likelihood of insignificance.
   * The regression coefficient (3.45) is significantly different from zero (p < 0.001).

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(3) Is the model useful?**

   * Check the **R-squared** near the bottom of the summary.
   * R-squared is a measure of the model's quality -- the fraction of the variance of y that is explained by the regression model. Bigger is better.

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(3) Is the model useful?**

   * The multiple R-squared (0.991) indicates that the model accounts for 99.1 percent of the variance in weights. The multiple R-squared is also the squared correlation between the actual and predicted value. (Further reading: Correlation and R-Squared [1](http://mathworld.wolfram.com/CorrelationCoefficient.html), [2](https://economictheoryblog.com/2014/11/05/the-coefficient-of-determination-latex-r2/), [3](http://www.win-vector.com/blog/2011/11/correlation-and-r-squared/).)
   * The adjusted value accounts for the number of variables in your model and so is a more realistic assessment of its effectiveness.

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(4) Does the model fit the data well?**

   * Plot the residuals and check the regression diagnostics.
   * The residual standard error (1.53 lbs.) can be thought of as the average error in predicting weight from height using this model. 

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```


--- .modal

## Plot the residuals

```{r, fig.height=4, fig.width=4}
## plot the residuals and check if the residuals appear to be approximately normal
plot(density(resid(fit)))
```


--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### Understand the summary

*** =left
**(5) Does the data satisfy the assumptions behind linear regression?**

   * Check whether the diagnostics confirm that a linear model is reasonable for your data.
   * If the residuals have a normal distribution, then the first quartile (1Q) and third quartile (3Q) should have about the same magnitude.

*** =right
```{r, echo=FALSE}
## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
```

--- .modal

## Plot the residuals

```{r, fig.height=4, fig.width=4}
qqnorm(resid(fit)) # a quantile normal plot 
qqline(resid(fit))
```

--- .modal

## Check statistical assumptions

```{r, fig.height=5, fig.width=5}
## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
```

--- &twocolvar w1:50% w2:50% .compact .scode

### Check statistical assumptions
*** =left
**Normality**
   * If the dependent variable is normally distributed for a fixed set of predictor values, then the residual values should be normally distributed with a mean of 0. 
   * The **Normal Q-Q plot** (upper right) is a probability plot of the standardized residuals against the values that would be expected under normality. If you've met the normality assumption, the points on this graph should fall on the straight 45-degree line. If they don't, you've clearly violated the normality assumption.

*** =right
```{r, echo=FALSE, fig.height=5, fig.width=5}
## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
```

--- &twocolvar w1:50% w2:50% .compact .scode

### Check statistical assumptions
*** =left
**Independence**
   * You have to use your understanding of how the data were collected. There's no a priori reason to believe that one woman's weight influences another woman's weight. If you found out that the data were sampled from families, you may have to adjust your assumption of independence.

*** =right
```{r, echo=FALSE, fig.height=5, fig.width=5}
## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
```

--- &twocolvar w1:50% w2:50% .compact .scode

### Check statistical assumptions
*** =left
**Linearity**
   * If the dependent variable is linearly related to the independent variables, there should be no systematic relationship between the residuals and the predicted (that is, fitted) values. 
   * In other words, the model should capture all the systematic variance present in the data, leaving nothing but random noise. 
   * In the **Residuals vs Fitted** graph (upper left), you see clear evidence of a curved relationship, which suggests that you may want to add a quadratic term to the regression.

*** =right
```{r, echo=FALSE, fig.height=5, fig.width=5}
## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
```

--- &twocolvar w1:50% w2:50% .compact .scode

### Check statistical assumptions
*** =left
**Homoscedasticity**
   * If you've met the constant variance assumption, the points in the **Scale-Location** graph (bottom left) should be a random band around a horizontal line. You seem to meet this assumption.

*** =right
```{r, echo=FALSE, fig.height=5, fig.width=5}
## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
```

--- #multiple-linear-regression .modal

## Multiple linear regression
```{r}
## we'll use the state.x77 dataset in the base package
states = as.data.frame(state.x77[,c("Murder", "Population",
                                     "Illiteracy", "Income", "Frost")])
head(states)
```

--- .modal .compact .scode

## Examining data 
```{r}
cor(states)
```

--- .modal

## Examining data 
```{r, fig.height=5, fig.width=5, warning=FALSE}
## examining bivariate relationships using 'scatterplotMatrix' in the 'car' package
scatterplotMatrix(states, spread=FALSE, lty.smooth=2,
                  main="Scatter Plot Matrix")
```

--- .scode .compact

## Multiple linear regression
```{r}
fit = lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### What does the result mean?  

*** =left
   * In multiple regression, the coefficients indicate the increase in the dependent variable for a unit change in a predictor variable, *holding all other predictor variables constant*. 
   * For example, the regression coefficient for Illiteracy is 4.14, suggesting that an increase of 1 percent in illiteracy is associated with a 4.14 percent increase in the murder rate, controlling for population, income, and temperature. 

*** =right
```{r, echo=FALSE}
fit = lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
summary(fit)
```

--- &twocolvar w1:40% w2:60% .compact .scode-nowrap

### What does the result mean?  

*** =left
   * The coefficient is significantly different from zero at the p < .0001 level. The coefficient for Frost isn't significantly different from zero (p = 0.954) suggesting that Frost and Murder aren't linearly related when controlling for the other predictor variables. 
   * Taken together, the predictor variables account for 57 percent of the variance in murder rates across states.

*** =right
```{r, echo=FALSE}
fit = lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
summary(fit)
```

--- #variable-selection .compact

## Variable selection
**Stepwise selection**: variables are added to or deleted from a model one at a time.
* forward: add predictor variables to the model one at a time, stopping when the addition of variables would no longer improve the model
* backward: start with a model that includes all predictor variables, and then delete them one at a time until removing variables would degrade the quality of the model

--- .scode

## Variable selection
```{r}
## backward stepwise selection
library(MASS)
fit1 = lm(Murder ~ Population + Illiteracy + Income + Frost,
          data=states)
stepAIC(fit, direction="backward")
```


--- &twocolvar w1:40% w2:60% .compact .sscode-nowrap

## Variable selection

*** =left
* You start with all four predictors in the model. For each step, the AIC column provides the model AIC resulting from the deletion of the variable listed in that row.
* Models with smaller AIC values (indicating adequate fit with fewer parameters) are preferred.
* Although stepwise selection may find a good model, there's no guarantee that it will find the best model because not every possible model is evaluated. (Alternative: all subsets method. See library `leaps`.)

*** =right
```{r, echo=FALSE}
## backward stepwise selection
library(MASS)
fit1 = lm(Murder ~ Population + Illiteracy + Income + Frost,
          data=states)
stepAIC(fit, direction="backward")
```

--- #model-performance .compact .scode

## Model performance
```{r}
## compute R-squared for the woman data
y = women$weight; x = women$height
fit = lm(y~x)
summary(fit)
```

--- .compact .scode

## Model performance
```{r}
## compute R-squared for the woman data
mean.mse = mean((rep(mean(y),length(y)) - y)^2)
model.mse = mean(residuals(fit)^2)
rmse = sqrt(model.mse)
rmse ## root mean square error
r2 = 1 - (model.mse / mean.mse)
r2
cor(y, fit$fitted.values)^2
```

--- .compact .scode

## Model performance
```{r}
## leave-one-out cross validation
n = length(women$weight)
error = dim(n)
for (k in 1:n) {
  train1 = c(1:n)
  train = train1[train1!=k] ## pick elements that are different from k
  m2 = lm(weight ~ height, data=women[train ,])
  pred = predict(m2, newdat=women[-train ,])
  obs = women$weight[-train]
  error[k] = obs-pred
}
me=mean(error)
me ## mean error
rmse=sqrt(mean(error^2))
rmse ## root mean square error (out-of-sample)
```


--- #polynomial-regression .compact .scode

## Polynomial regression
```{r}
fit2 = lm(weight ~ height + I(height^2), data=women)
summary(fit2)
```

--- .compact .scode

## Polynomial regression
```{r, fig.height=4, fig.width=4}
plot(women$height,women$weight,
     xlab="Height (in inches)",
     ylab="Weight (in lbs)")
lines(women$height,fitted(fit2))
```


--- &twocolvar w1:48% w2:50% .compact .sscode-nowrap

## Polynomial regression

*** =left
$$ \hat{weight}= 261.88- 7.35\times height +0.083 \times height^2 $$
Both regression coefficients are significant at the p < 0.0001 level. The amount of variance accounted for has increased to 99.9 percent. The significance of the squared term (t = 13.89, p < .001) suggests that inclusion of the quadratic term improves the model fit.

*** =right
```{r, echo=FALSE}
fit2 = lm(weight ~ height + I(height^2), data=women)
summary(fit2)
```

--- #non-linear .modal 

## Non-linear Data
```{r, fig.height=4, fig.width=4}
## create testing data (sine wave)
set.seed(1)
x = seq(0, 1, by = 0.01)
y = sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
df = data.frame(X = x, Y = y)
ggplot(df, aes(x = X, y = Y)) + geom_point()
```

--- .scode 

## Non-linear Data
```{r, fig.height=4, fig.width=4}
## fit with linear regression
summary(lm(Y ~ X, data = df)) # explain 60% of the variance
```

--- .scode

## Non-linear Data
```{r, fig.height=4, fig.width=4}
ggplot(data.frame(X = x, Y = y), aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

--- .scode 

## Non-linear Data
```{r, fig.height=4, fig.width=4}
## add new features
df <- transform(df, X2 = X ^ 2)
df <- transform(df, X3 = X ^ 3)
summary(lm(Y ~ X + X2 + X3, data = df)) # R2 from 60% to 97%, by adding two more inputs
```

--- .sscode-nowrap .compact

## Non-linear Data
```{r, fig.height=4, fig.width=4}
## add more features
df <- transform(df, X4 = X ^ 4)
df <- transform(df, X5 = X ^ 5)
df <- transform(df, X6 = X ^ 6)
df <- transform(df, X7 = X ^ 7)
df <- transform(df, X8 = X ^ 8)
df <- transform(df, X9 = X ^ 9)
df <- transform(df, X10 = X ^ 10)
df <- transform(df, X11 = X ^ 11)
df <- transform(df, X12 = X ^ 12)
df <- transform(df, X13 = X ^ 13)
df <- transform(df, X14 = X ^ 14)
df <- transform(df, X15 = X ^ 15)
summary(lm(Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 , data = df))
```

--- &twocolvar w1:30% w2:70% .compact .sscode-nowrap

## Non-linear Data

*** =left
* new feature correlated with the old columns
* the linear regression breaks down and can't find coefficients for all of the columns separately

*** =right
```{r, echo=FALSE, fig.height=4, fig.width=4}
## add more features
df <- transform(df, X4 = X ^ 4)
df <- transform(df, X5 = X ^ 5)
df <- transform(df, X6 = X ^ 6)
df <- transform(df, X7 = X ^ 7)
df <- transform(df, X8 = X ^ 8)
df <- transform(df, X9 = X ^ 9)
df <- transform(df, X10 = X ^ 10)
df <- transform(df, X11 = X ^ 11)
df <- transform(df, X12 = X ^ 12)
df <- transform(df, X13 = X ^ 13)
df <- transform(df, X14 = X ^ 14)
df <- transform(df, X15 = X ^ 15)
summary(lm(Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 , data = df))
```

--- .sscode-nowrap .compact

## Non-linear Data

```{r, fig.height=4, fig.width=4}
## use 'poly' function
## similar to X + X^2 + X^3 + ... + X^14, but with orthogonal columns

summary(lm(Y ~ poly(X, degree = 14), data = df))
```

--- .sscode-nowrap .compact

## Plot Non-linear Data

```{r, fig.height=4, fig.width=4}
## restore testing data
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
df0 <- data.frame(X = x, Y = y)
## using poly with degrees of 1, 3, 5, and 25
poly.fit <- lm(Y ~ poly(X, degree = 1), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
ggplot(df0 , aes(x = X, y = Y)) +
  geom_point() + geom_line(data=df, aes(x = X, y = PredictedY))
```

--- .sscode-nowrap .compact

## Plot Non-linear Data (degree=3)

```{r, fig.height=4, fig.width=4}
poly.fit <- lm(Y ~ poly(X, degree = 3), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
ggplot(df0 , aes(x = X, y = Y)) +
  geom_point() + geom_line(data=df, aes(x = X, y = PredictedY))
```

--- .sscode-nowrap .compact

## Plot Non-linear Data (degree=5)

```{r, fig.height=4, fig.width=4}
poly.fit <- lm(Y ~ poly(X, degree = 5), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
ggplot(df0 , aes(x = X, y = Y)) +
  geom_point() + geom_line(data=df, aes(x = X, y = PredictedY))
```

--- .sscode-nowrap .compact

## Plot Non-linear Data (degree=25)

```{r, fig.height=4, fig.width=4}
poly.fit <- lm(Y ~ poly(X, degree = 25), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
ggplot(df0 , aes(x = X, y = Y)) +
  geom_point() + geom_line(data=df, aes(x = X, y = PredictedY))
```

model become too complex (too many parameters) and exaggerate minor fluctuations (noise) in the data

--- &twocolvar w1:70% w2:30% #cross-validation .sscode-nowrap .compact
## Cross-validation

*** =left
```{r, fig.show = 'hide'}
## split our data into a training set and test set
n <- length(x)
indices <- sort(sample(1:n, round (0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
## use RMSE to measure the performance
rmse <- function(y, h) {
  return(sqrt(mean((y - h) ^ 2)))
}
## loop over a set of polynomial degrees (from 1 to 12)
performance <- data.frame()
for (d in 1:12) {
  poly.fit <- lm(Y ~ poly(X, degree = d), data = training.df)
  performance <- rbind(performance ,
                       data.frame(Degree = d,
                                  Data = 'Training ',
                                  RMSE = rmse(training.y, predict(poly.fit))))
  performance <- rbind(performance ,
                       data.frame(Degree = d,
                                  Data = 'Test ',
                                  RMSE = rmse(test.y, predict(poly.fit ,
                                                              newdata = test.df))))
}
## plot the performance of the polynomial regression models for all the degrees
ggplot(performance , aes(x = Degree , y = RMSE , linetype = Data)) +
  geom_point() +
  geom_line()
```

*** =right
```{r, echo=FALSE,fig.height=3, fig.width=4}
## plot the performance of the polynomial regression models for all the degrees
ggplot(performance , aes(x = Degree , y = RMSE , linetype = Data)) +
  geom_point() +
  geom_line()
```

--- #regularization .sscode-nowrap .compact
## Regularization


```{r,message = FALSE}
lm.fit <- lm(y ~ x)
l2.model.complexity <- sum(coef(lm.fit) ^ 2) # L2 norm (sum of squaring values)
l1.model.complexity <- sum(abs(coef(lm.fit))) # L1 norm (sum of absolute values)
library('glmnet')
x <- matrix(x) ## input of glmnet has to be in matrix form
glmnet(poly(x, degree = 2), y)
```

--- &twocolvar w1:70% w2:30% .sscode-nowrap .compact
## Regularization

*** =left
```{r, fig.show="hide"}
## get an entire set of possible regularizations (from the strongest to the weakest).
## Df: how many nonzero coefficients in the model;
## %Dev: the R2 for this model; 
## Lambda: controls how complex the model is (a.k.a. hyperparameter)

## split it into a training set and a test set
n <- length(x)
indices <- sort(sample(1:n, round (0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
df <- data.frame(X = x, Y = y)
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
rmse <- function(y, h) {
  return(sqrt(mean((y - h) ^ 2)))
}
## loop over values of Lambda
glmnet.fit <- with(training.df, glmnet(poly(X, degree = 10), Y))
lambdas <- glmnet.fit$lambda ## regularization parameter (sequence)
performance <- data.frame()
for (lambda in lambdas) {
  performance <- rbind(performance ,
                       data.frame(Lambda = lambda ,
                                  RMSE = rmse(test.y, with(test.df, predict(glmnet.fit , poly(X, degree = 10), s = lambda)))))
}
## plot to see the model performance w.r.t. the range of lambdas
ggplot(performance , aes(x = Lambda , y = RMSE)) +
  geom_point() +
  geom_line()
```

*** =right
```{r, echo=FALSE,fig.height=4, fig.width=4}
## plot to see the model performance w.r.t. the range of lambdas
ggplot(performance , aes(x = Lambda , y = RMSE)) +
  geom_point() +
  geom_line()
```

--- .sscode-nowrap .compact
## Regularization

```{r, fig.height=4, fig.width=4}
## we get the best possible performance with Lambda near 0.05; select that value and train our model
## using only 3 nonzero coefficients instead of 10

best.lambda <- with(performance , Lambda[which(RMSE == min(RMSE))])
glmnet.fit <- with(df, glmnet(poly(X, degree = 10), Y))
## use coef to examine the structure of our regularized model
coef(glmnet.fit , s = best.lambda)
```

--- #local-poly .scode-nowrap .compact
## Local Polynomial Regression

```{r,autodep=TRUE}
library(locfit)

## first we read in the data
data.url = 'http://www.yurulin.com/class/spring2014_datamining/data/data_text'
ethanol <- read.csv(sprintf ("%s/ethanol.csv",data.url))
colnames(ethanol) <- c('NOx','C','E')
ethanol [1:3,]
```

--- .scode-nowrap .compact
## Local Polynomial Regression

```{r, fig.height=4, fig.width=4}
## standard regression of NOx on the equivalence ratio dosen't work
fitreg=lm(NOx~E,data=ethanol)
plot(NOx~E,data=ethanol)
abline(fitreg)
```

--- .scode-nowrap .compact
## Local Polynomial Regression

```{r, fig.height=4, fig.width=4, autodep=TRUE}
## local polynomial regression of NOx on the equivalence ratio
## fit with a 50% nearest neighbor bandwidth.
fit <- locfit(NOx~lp(E,nn=0.5),data=ethanol)
plot(fit)
```

--- .scode-nowrap .compact
## Local Polynomial Regression

```{r, fig.height=4, fig.width=4, autodep=TRUE}
## cross-validation
alpha <-seq(0.20,1,by=0.01)
n1=length(alpha)
g=matrix(nrow=n1,ncol=4)
for (k in 1:length(alpha)) {
  g[k,]<-gcv(NOx~lp(E,nn=alpha[k]),data=ethanol)
}
head(g) 
```


--- .scode-nowrap .compact
## Local Polynomial Regression

```{r, fig.height=4, fig.width=4, autodep=TRUE, dependson=c(-1)}
plot(g[,4]~g[,3],ylab="GCV",xlab="degrees of freedom")
```


--- .scode-nowrap .compact
## Local Polynomial Regression

```{r, fig.height=4, fig.width=4, autodep=TRUE}
f1=locfit(NOx~lp(E,nn=0.30),data=ethanol)
f1; plot(f1)
```

--- #lasso .scode-nowrap .compact
## LASSO

```{r, fig.height=4, fig.width=4}
data.url = 'http://www.yurulin.com/class/spring2014_datamining/data/data_text'
prostate <- read.csv(sprintf ("%s/prostate.csv",data.url))
prostate [1:3,]
```

--- .scode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
m1=lm(lcavol~.,data=prostate)
summary(m1)
```

--- .scode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
## the model.matrix statement defines the model to be fitted
x <- model.matrix(lcavol~age+lbph+lcp+gleason+lpsa ,data=prostate)
x=x[,-1] # stripping off the column of 1s as LASSO includes the intercept automatically

library(lars)
## lasso on all data
lasso <- lars(x=x,y=prostate$lcavol ,trace=TRUE)
```

--- .scode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
## trace of lasso (standardized) coefficients for varying penalty
plot(lasso)
```

--- .scode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
lasso
coef(lasso ,s=c(.25,.50,0.75,1.0),mode="fraction")
```

--- .scode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
## cross -validation using 10 folds
cv.lars(x=x,y=prostate$lcavol ,K=10)
```

--- .sscode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
## another way to evaluate lasso's out-of-sample prediction performance
MSElasso25=dim(10)
MSElasso50=dim(10)
MSElasso75=dim(10)
MSElasso100=dim(10)
set.seed(1)
for(i in 1:10){
  train <- sample(1:nrow(prostate),80)
  lasso <- lars(x=x[train ,],y=prostate$lcavol[train])
  MSElasso25[i]=
    mean((predict(lasso ,x[-train ,],s=.25,mode="fraction")$fit -prostate$lcavol[-train ])^2)
  MSElasso50[i]=
    mean((predict(lasso ,x[-train ,],s=.50,mode="fraction")$fit -prostate$lcavol[-train ])^2)
  MSElasso75[i]=
    mean((predict(lasso ,x[-train ,],s=.75,mode="fraction")$fit -prostate$lcavol[-train ])^2)
  MSElasso100[i]=
    mean((predict(lasso ,x[-train ,],s=1.00,mode="fraction")$fit -prostate$lcavol[-train ])^2)
}
mean(MSElasso25)
mean(MSElasso50)
mean(MSElasso75)
mean(MSElasso100)
```

--- .sscode-nowrap .compact
## LASSO
```{r, warning=FALSE, dependson=c(-1), fig.height=4, fig.width=4}
boxplot(MSElasso25 ,MSElasso50 ,MSElasso75 ,MSElasso100 ,
        ylab="MSE", sub="LASSO model",
        xlab="s=0.25 s=0.50 s=0.75 s=1.0(LS)")
```
