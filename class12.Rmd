---
title       : Class12
subtitle    : Data Processing and Features
author      : Yu-Ru Lin
job         : 
framework   : shower        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
toc         : true
toc_depth   : 2

--- #toc
## Class12
  
* [Set up](#set-up)
* [Big data set](#big)
* [Sparse matrix](#sparse)
* [Feature selection](#featureselection)
* [Frequent patterns](#freq)

--- #set-up .modal 

## Install R packages
```{r, eval=FALSE}
## this tutorial uses the following packages
install.packages('sqldf')
install.packages('FSelector')
```

--- .scode-nowrap .compact #big
## Big data set

* When dealing with big data files, do not read everything into memory.
* We use the sample data from: http://grouplens.org/datasets/hetrec-2011/
* See: http://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-readme.txt
  
```{r}
## give the input filename
data.path = 'hetrec2011'
ifilename = sprintf('%s/movies.dat',data.path)
## read the first n=3 lines
lines = readLines(ifilename,n=3)
lines
```

--- .ssscode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
## read the first 3 rows into a data.frame object
df = read.csv(ifilename, sep='\t', nrows=3) ## we saw from the first few lines that the field separator is <TAB>
df
```

--- .ssscode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
## read row 101--103
df = read.csv(ifilename, sep='\t', nrows=3, skip=101, header=F) ## if we set 'header=T', the first non-skip row will be treated as header (which is incorrect)
df
```

--- .scode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
## count how many lines in the file
# length(readLines(ifilename)) ## not recommended because it reads the file in memory
system(sprintf("wc -l %s",ifilename), intern=T ) 
## you can use 'wc' command on unix-based machine (linux, mac, etc.)

suppressWarnings(suppressPackageStartupMessages( ## avoid printing warnings
  require(R.utils)
)) ## assumig you have installed R.utils
countLines(ifilename)
```

--- .scode .compact 
## Big data set
```{r, dependson=c(-1)}
suppressWarnings(suppressPackageStartupMessages( ## avoid printing warnings
  require(sqldf)
)) ## assumig you have installed sqldf
read.csv.sql(ifilename, "select count(*) from file", sep='\t')
```

--- .scode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
sql = 'select id,title,year from file limit 1' ## read 1 row
df = read.csv.sql(ifilename, sql, sep='\t')
df
```

--- .scode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
sql = 'select id,title,year from file limit 10' ## read 10 rows
df = read.csv.sql(ifilename, sql, sep='\t')
df
```

--- .scode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
sql = 'select id,title,year from file limit 5,10' 
## skip the first 5 rows, and read the next 10 rows
df = read.csv.sql(ifilename, sql, sep='\t')
df
```

--- .scode-nowrap .compact 
## Big data set
```{r, dependson=c(-1)}
sql = 'select year,count(*) from file group by year limit 10' 
## you can do more if you are familiar with SQL 
df = read.csv.sql(ifilename, sql, sep='\t')
df
```

* if you have mysql database, you can use 'RMySQL' package.
* Other tips (using 'fread'): http://davetang.org/muse/2013/09/03/handling-big-data-in-r/


--- .scode-nowrap .compact #sparse
## Sparse matrix
When dealing with big matrix, use sparse format to construct matrix.

```{r, dependson=c(-1)}
ifilename = sprintf('%s/movies.dat',data.path)
movie.df = read.csv(ifilename, sep='\t')
dim(movie.df)
```

--- .scode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
movie.df[1:3,1:4] ## check a small part
```

--- .scode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
ifilename = sprintf('%s/%s',data.path, 'movie_genres.dat')
m2g.df = read.csv(ifilename, sep='\t', header=T)
m2g.df[1:3,]
dim(m2g.df)
```

--- .scode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
m2g.df[1:10,] ## check a small part
```

--- .sscode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
## create a numeric index for genres, so that we can create a matrix
g = unique(as.character(m2g.df$genre))
length(g) ## how many genres?
m = unique(as.character(m2g.df$movieID))
length(m) ## how many movies?
idx2g = match(as.character(m2g.df$genre),g)
idx2g[1:20]; length(idx2g)
idx2m = match(as.character(m2g.df$movieID),m)
idx2m[1:20]; length(idx2m)
```

--- .sscode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
require(Matrix);require(MASS)
## construct a movie-to-genre sparse matrix; the format is (i,j,v), meaning entry(i,j)=v
m2g = sparseMatrix(i=idx2m,j=idx2g,x=rep(1,nrow(m2g.df)) )
dim(m2g)
object.size(m2g)
print(object.size(m2g),units="Mb")  ## gives you size of the object in Mb

## report current memory used by R, e.g., __ Mb in my machine
# print(object.size(x=lapply(ls(), get)), units="Mb") 
denseM2G = as.matrix(m2g) ## dense matrix
print(object.size(denseM2G),units="Mb") 

## report current memory used by R, e.g., __ Mb in my machine
print(object.size(x=lapply(ls(), get)), units="Mb") 

rm(denseM2G)
## report current memory used by R, e.g., __ Mb in my machine
print(object.size(x=lapply(ls(), get)), units="Mb")
```

--- .scode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
m2g[1:3,]
```

--- .sscode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
S = svd(m2g, nu=3, nv=3)
S$d;dim(S$u);dim(S$v)
```

--- .scode-nowrap .compact 
## Sparse matrix
```{r, dependson=c(-1)}
## if the matrix is very big, you can use package irlba to compute approximate SVD
## this has been tested in Netflix matrix!
require(irlba)
S = irlba(m2g, nu=3, nv=3)
S$d;dim(S$u);dim(S$v)
system.time({S = irlba(m2g, nu=5, nv=5)}) ## report the running time
```

--- .scode-nowrap .compact #featureselection
## Feature selection

* With feature selection, you can reduce the dimension and complexity of future steps on the Data Mining process.
* Examples adopted from: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection

```{r, dependson=c(-1)}
require(FSelector) ## assumig you have installed FSelector
## installation note: requires Java installed on your machine and the rJava package

## load a dataset and use it as the main source of data
require(mlbench)
data(HouseVotes84)
HouseVotes84[1:3,]
```

--- .ssscode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## Feature Ranking --
## calculate weights for each atribute using some function
weights = chi.squared(Class~., HouseVotes84)
## the weights can be given by different functions: Pearson's correlation (linear.correlation), Spearman's correlation (rank.correlation), Chi-squared Filter (chi.squared), Information Gain (information.gain), etc.
print(weights)
```

--- .scode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## select a subset of 5 features with the lowest weight
subset = cutoff.k(weights, 5)

## print the results
f = as.simple.formula(subset, "Class")
print(f)
```

--- .scode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## Feature Subset Selection (Wrappers) --
library(rpart)
data(iris)
iris[1:3,]
```

--- .ssscode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## define the evaluation function  
evaluator <- function(subset) {
  #here you must define a function that returns a double value to evaluate the given subset
  #consider high values for good evaluation and low values for bad evaluation.
  #k-fold cross validation
  k <- 5
  splits <- runif(nrow(iris))
  results = sapply(1:k, function(i) {
    test.idx <- (splits >= (i - 1) / k) & (splits < i / k)
    train.idx <- !test.idx
    test <- iris[test.idx, , drop=FALSE]
    train <- iris[train.idx, , drop=FALSE]
    tree <- rpart(as.simple.formula(subset, "Species"), train)
    error.rate = sum(test$Species != predict(tree, test, type="c")) / nrow(test)
    return(1 - error.rate)
  })
  print(as.simple.formula(subset, "Species"))
#   print(subset)
  print(mean(results))
  return(mean(results))
}
```

--- .ssscode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## perform the best subset search
subset = best.first.search(names(iris)[-5], evaluator)
```

--- .scode-nowrap .compact 
## Feature selection
```{r, dependson=c(-1)}
## you can use different strategy for search the optimal subset: Best First Search (best.first.search), Exhaustive Search (exhaustive.search), Greedy Search (forward.search, backward.search), etc.

## prints the result
f = as.simple.formula(subset, "Species")
print(f)
```

--- .scode-nowrap .compact #freq
## Frequent patterns
```{r, dependson=c(-1)}
library(arules) ## assumig you have installed arules
cat(readLines("sample-data/toy-transaction.txt"),sep='\n') 
## see what's inside the toy transactions
```

--- .scode-nowrap .compact 
## Frequent patterns
```{r, dependson=c(-1)}
```

--- .scode-nowrap .compact 
## Frequent patterns
```{r, dependson=c(-1)}
```

--- .scode-nowrap .compact 
## Frequent patterns
```{r, dependson=c(-1)}
```
