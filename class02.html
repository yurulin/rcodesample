<!DOCTYPE HTML>
<html lang="en-US">
<head>
	<title>Class02</title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=1274, user-scalable=no">
	<meta name="description" content="Class02">
	<meta name="author" content="Yu-Ru Lin">
	<meta name="generator" content="slidify" />
	<!-- LOAD STYLE SHEETS -->
	<link rel="stylesheet" href="libraries/frameworks/shower/themes/ribbon/styles/screen.css">
	<link rel="stylesheet" media="print"
	  href="libraries/frameworks/shower/themes/ribbon/styles/print.css">
	<link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css">  <link rel="stylesheet" href = "assets/css/mystyle.css">
<link rel="stylesheet" href = "assets/css/ribbons.css">

	<!--
		To apply styles to the certain slides
		use slide ID to get needed elements
		-->
	<style>
		#Cover h2 {
      margin:65px 0 0;
			color:#FFF;
			text-align:center;
			font-size:70px;
			}
		#FitToWidth h2,
		#FitToHeight h2 {
			color:#FFF;
			text-align:center;
			}
	</style> 
</head>
<body class="list">
  <header class="caption">
  	<h1>Class02</h1>
	</header>
  <section class="slide " id="toc">
  <div>
    <h2>Class02</h2>
    <ul>
<li><a href="#set-up">Set up</a></li>
<li><a href="#simple-regression">Simple Regression</a></li>
<li><a href="#multiple-linear-regression">Multiple linear regression</a></li>
<li><a href="#variable-selection">Variable selection</a></li>
<li><a href="#model-performance">Model performance</a></li>
<li><a href="#polynomial-regression">Polynomial regression</a></li>
<li><a href="#non-linear">Non-linear Data</a></li>
<li><a href="#cross-validation">Cross-validation</a></li>
<li><a href="#regularization">Regularization</a></li>
<li><a href="#local-poly">Local Polynomial Regression</a></li>
<li><a href="#lasso">LASSO</a></li>
</ul>

  </div>
</section>
<section class="slide modal" id="set-up">
  <div>
    <h2>Install R packages</h2>
    
  </div>
</section>
<section class="slide scode compact" id="simple-regression">
  <div>
    <h2>Simple Regression</h2>
    <p>The examples are taken from <a href="http://www.manning.com/kabacoff/">R in Action</a>
\[\hat{y}=\alpha +\beta x\]</p>

<pre><code>##    height weight
## 1      58    115
## 2      59    117
## 3      60    120
## 4      61    123
## 5      62    126
## 6      63    129
## 7      64    132
## 8      65    135
## 9      66    139
## 10     67    142
## 11     68    146
## 12     69    150
## 13     70    154
## 14     71    159
## 15     72    164
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-4">
  <div>
    <h2>Simple Regression</h2>
    <pre><code>##      height         weight     
##  Min.   :58.0   Min.   :115.0  
##  1st Qu.:61.5   1st Qu.:124.5  
##  Median :65.0   Median :135.0  
##  Mean   :65.0   Mean   :136.7  
##  3rd Qu.:68.5   3rd Qu.:148.0  
##  Max.   :72.0   Max.   :164.0
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-5">
  <div>
    <h2>Simple Regression</h2>
    <p><img src="assets/fig/class02-chunk-4-1.png" alt="plot of chunk class02-chunk-4"></p>

  </div>
</section>
<section class="slide modal" id="slide-6">
  <div>
    <h2>Simple Regression</h2>
    <p><img src="assets/fig/class02-chunk-5-1.png" alt="plot of chunk class02-chunk-5"></p>

  </div>
</section>
<section class="slide modal" id="slide-7">
  <div>
    <h2>Simple Regression</h2>
    <p><img src="assets/fig/class02-chunk-6-1.png" alt="plot of chunk class02-chunk-6"></p>

  </div>
</section>
<section class="slide scode" id="slide-8">
  <div>
    <h2>Simple Regression</h2>
    <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-9">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<p>\[ \hat{weight}=-87.52+3.45\times height \]</p>

<ul>
<li>There&#39;s an expected increase of 3.45 pounds of weight for every 1 inch increase in height. </li>
<li>The intercept is merely an adjustment constant.<br></li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-10">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(1) Is the model statistically significant?</strong></p>

<ul>
<li>Check the <strong>F statistic</strong> at the bottom of the summary.</li>
<li>The F statistic tells you whether the model is insignificant or significant. Big p-value indicates a high likelihood of insignificance. (Further reading: F-statistic <a href="http://www.statisticshowto.com/f-statistic/">1</a>, <a href="https://onlinecourses.science.psu.edu/stat501/node/295">2</a>.)</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-11">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(2) Are the coefficients significant?</strong></p>

<ul>
<li>Check the coefficient&#39;s <strong>t statistics</strong> and <strong>p-values</strong> in the summary, or check their confidence intervals.</li>
<li>If a variable&#39;s coefficient is zero then the variable is worthless; it adds nothing to the model. </li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-12">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(2) Are the coefficients significant?</strong></p>

<ul>
<li>The p-value is a probability that the coefficient is not significant. Big is bad because it indicates a high likelihood of insignificance.</li>
<li>The regression coefficient (3.45) is significantly different from zero (p &lt; 0.001).</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-13">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(3) Is the model useful?</strong></p>

<ul>
<li>Check the <strong>R-squared</strong> near the bottom of the summary.</li>
<li>R-squared is a measure of the model&#39;s quality -- the fraction of the variance of y that is explained by the regression model. Bigger is better.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-14">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(3) Is the model useful?</strong></p>

<ul>
<li>The multiple R-squared (0.991) indicates that the model accounts for 99.1 percent of the variance in weights. The multiple R-squared is also the squared correlation between the actual and predicted value. (Further reading: Correlation and R-Squared <a href="http://mathworld.wolfram.com/CorrelationCoefficient.html">1</a>, <a href="https://economictheoryblog.com/2014/11/05/the-coefficient-of-determination-latex-r2/">2</a>, <a href="http://www.win-vector.com/blog/2011/11/correlation-and-r-squared/">3</a>.)</li>
<li>The adjusted value accounts for the number of variables in your model and so is a more realistic assessment of its effectiveness.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-15">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(4) Does the model fit the data well?</strong></p>

<ul>
<li>Plot the residuals and check the regression diagnostics.</li>
<li>The residual standard error (1.53 lbs.) can be thought of as the average error in predicting weight from height using this model. </li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="slide-16">
  <div>
    <h2>Plot the residuals</h2>
    <p><img src="assets/fig/class02-chunk-15-1.png" alt="plot of chunk class02-chunk-15"></p>

  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-17">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>(5) Does the data satisfy the assumptions behind linear regression?</strong></p>

<ul>
<li>Check whether the diagnostics confirm that a linear model is reasonable for your data.</li>
<li>If the residuals have a normal distribution, then the first quartile (1Q) and third quartile (3Q) should have about the same magnitude.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="slide-18">
  <div>
    <h2>Plot the residuals</h2>
    <p><img src="assets/fig/class02-chunk-17-1.png" alt="plot of chunk class02-chunk-17"></p>

  </div>
</section>
<section class="slide modal" id="slide-19">
  <div>
    <h2>Check statistical assumptions</h2>
    <p><img src="assets/fig/class02-chunk-18-1.png" alt="plot of chunk class02-chunk-18"></p>

  </div>
</section>
<section class="slide compact scode" id="slide-20">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Normality</strong></p>

<ul>
<li>If the dependent variable is normally distributed for a fixed set of predictor values, then the residual values should be normally distributed with a mean of 0. </li>
<li>The <strong>Normal Q-Q plot</strong> (upper right) is a probability plot of the standardized residuals against the values that would be expected under normality. If you&#39;ve met the normality assumption, the points on this graph should fall on the straight 45-degree line. If they don&#39;t, you&#39;ve clearly violated the normality assumption.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/class02-chunk-19-1.png" alt="plot of chunk class02-chunk-19"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-21">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Independence</strong></p>

<ul>
<li>You have to use your understanding of how the data were collected. There&#39;s no a priori reason to believe that one woman&#39;s weight influences another woman&#39;s weight. If you found out that the data were sampled from families, you may have to adjust your assumption of independence.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/class02-chunk-20-1.png" alt="plot of chunk class02-chunk-20"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-22">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Linearity</strong></p>

<ul>
<li>If the dependent variable is linearly related to the independent variables, there should be no systematic relationship between the residuals and the predicted (that is, fitted) values. </li>
<li>In other words, the model should capture all the systematic variance present in the data, leaving nothing but random noise. </li>
<li>In the <strong>Residuals vs Fitted</strong> graph (upper left), you see clear evidence of a curved relationship, which suggests that you may want to add a quadratic term to the regression.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/class02-chunk-21-1.png" alt="plot of chunk class02-chunk-21"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-23">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Homoscedasticity</strong></p>

<ul>
<li>If you&#39;ve met the constant variance assumption, the points in the <strong>Scale-Location</strong> graph (bottom left) should be a random band around a horizontal line. You seem to meet this assumption.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/class02-chunk-22-1.png" alt="plot of chunk class02-chunk-22"></p>

</div>
  </div>
</section>
<section class="slide modal" id="multiple-linear-regression">
  <div>
    <h2>Multiple linear regression</h2>
    <pre><code>##            Murder Population Illiteracy Income Frost
## Alabama      15.1       3615        2.1   3624    20
## Alaska       11.3        365        1.5   6315   152
## Arizona       7.8       2212        1.8   4530    15
## Arkansas     10.1       2110        1.9   3378    65
## California   10.3      21198        1.1   5114    20
## Colorado      6.8       2541        0.7   4884   166
</code></pre>

  </div>
</section>
<section class="slide modal compact scode" id="slide-25">
  <div>
    <h2>Examining data</h2>
    <pre><code>##                Murder Population Illiteracy     Income      Frost
## Murder      1.0000000  0.3436428  0.7029752 -0.2300776 -0.5388834
## Population  0.3436428  1.0000000  0.1076224  0.2082276 -0.3321525
## Illiteracy  0.7029752  0.1076224  1.0000000 -0.4370752 -0.6719470
## Income     -0.2300776  0.2082276 -0.4370752  1.0000000  0.2262822
## Frost      -0.5388834 -0.3321525 -0.6719470  0.2262822  1.0000000
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-26">
  <div>
    <h2>Examining data</h2>
    <p><img src="assets/fig/class02-chunk-25-1.png" alt="plot of chunk class02-chunk-25"></p>

  </div>
</section>
<section class="slide scode compact" id="slide-27">
  <div>
    <h2>Multiple linear regression</h2>
    <pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-28">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<ul>
<li>In multiple regression, the coefficients indicate the increase in the dependent variable for a unit change in a predictor variable, <em>holding all other predictor variables constant</em>. </li>
<li>For example, the regression coefficient for Illiteracy is 4.14, suggesting that an increase of 1 percent in illiteracy is associated with a 4.14 percent increase in the murder rate, controlling for population, income, and temperature. </li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-29">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<ul>
<li>The coefficient is significantly different from zero at the p &lt; .0001 level. The coefficient for Frost isn&#39;t significantly different from zero (p = 0.954) suggesting that Frost and Murder aren&#39;t linearly related when controlling for the other predictor variables. </li>
<li>Taken together, the predictor variables account for 57 percent of the variance in murder rates across states.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

</div>
  </div>
</section>
<section class="slide compact" id="variable-selection">
  <div>
    <h2>Variable selection</h2>
    <p><strong>Stepwise selection</strong>: variables are added to or deleted from a model one at a time.</p>

<ul>
<li>forward: add predictor variables to the model one at a time, stopping when the addition of variables would no longer improve the model</li>
<li>backward: start with a model that includes all predictor variables, and then delete them one at a time until removing variables would degrade the quality of the model</li>
</ul>

  </div>
</section>
<section class="slide scode" id="slide-31">
  <div>
    <h2>Variable selection</h2>
    <pre><code>## Start:  AIC=97.75
## Murder ~ Population + Illiteracy + Income + Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Frost       1     0.021 289.19  95.753
## - Income      1     0.057 289.22  95.759
## &lt;none&gt;                    289.17  97.749
## - Population  1    39.238 328.41 102.111
## - Illiteracy  1   144.264 433.43 115.986
## 
## Step:  AIC=95.75
## Murder ~ Population + Illiteracy + Income
## 
##              Df Sum of Sq    RSS     AIC
## - Income      1     0.057 289.25  93.763
## &lt;none&gt;                    289.19  95.753
## - Population  1    43.658 332.85 100.783
## - Illiteracy  1   236.196 525.38 123.605
## 
## Step:  AIC=93.76
## Murder ~ Population + Illiteracy
## 
##              Df Sum of Sq    RSS     AIC
## &lt;none&gt;                    289.25  93.763
## - Population  1    48.517 337.76  99.516
## - Illiteracy  1   299.646 588.89 127.311
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy, data = states)
## 
## Coefficients:
## (Intercept)   Population   Illiteracy  
##   1.6515497    0.0002242    4.0807366
</code></pre>

  </div>
</section>
<section class="slide compact sscode-nowrap" id="slide-32">
  <div>
    <h2>Variable selection</h2>
      
<div class='left' style='float:left;width:40%'>
<ul>
<li>You start with all four predictors in the model. For each step, the AIC column provides the model AIC resulting from the deletion of the variable listed in that row.</li>
<li>Models with smaller AIC values (indicating adequate fit with fewer parameters) are preferred.</li>
<li>Although stepwise selection may find a good model, there&#39;s no guarantee that it will find the best model because not every possible model is evaluated. (Alternative: all subsets method. See library <code>leaps</code>.)</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## Start:  AIC=97.75
## Murder ~ Population + Illiteracy + Income + Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Frost       1     0.021 289.19  95.753
## - Income      1     0.057 289.22  95.759
## &lt;none&gt;                    289.17  97.749
## - Population  1    39.238 328.41 102.111
## - Illiteracy  1   144.264 433.43 115.986
## 
## Step:  AIC=95.75
## Murder ~ Population + Illiteracy + Income
## 
##              Df Sum of Sq    RSS     AIC
## - Income      1     0.057 289.25  93.763
## &lt;none&gt;                    289.19  95.753
## - Population  1    43.658 332.85 100.783
## - Illiteracy  1   236.196 525.38 123.605
## 
## Step:  AIC=93.76
## Murder ~ Population + Illiteracy
## 
##              Df Sum of Sq    RSS     AIC
## &lt;none&gt;                    289.25  93.763
## - Population  1    48.517 337.76  99.516
## - Illiteracy  1   299.646 588.89 127.311
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy, data = states)
## 
## Coefficients:
## (Intercept)   Population   Illiteracy  
##   1.6515497    0.0002242    4.0807366
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode" id="model-performance">
  <div>
    <h2>Model performance</h2>
    <pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## x             3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-34">
  <div>
    <h2>Model performance</h2>
    <pre><code>## [1] 1.419703
</code></pre>

<pre><code>## [1] 0.9910098
</code></pre>

<pre><code>## [1] 0.9910098
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-35">
  <div>
    <h2>Model performance</h2>
    <pre><code>## [1] 0.1148222
</code></pre>

<pre><code>## [1] 1.743782
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="polynomial-regression">
  <div>
    <h2>Polynomial regression</h2>
    <pre><code>## 
## Call:
## lm(formula = weight ~ height + I(height^2), data = women)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.50941 -0.29611 -0.00941  0.28615  0.59706 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 261.87818   25.19677  10.393 2.36e-07 ***
## height       -7.34832    0.77769  -9.449 6.58e-07 ***
## I(height^2)   0.08306    0.00598  13.891 9.32e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3841 on 12 degrees of freedom
## Multiple R-squared:  0.9995, Adjusted R-squared:  0.9994 
## F-statistic: 1.139e+04 on 2 and 12 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-37">
  <div>
    <h2>Polynomial regression</h2>
    <p><img src="assets/fig/class02-chunk-35-1.png" alt="plot of chunk class02-chunk-35"></p>

  </div>
</section>
<section class="slide compact sscode-nowrap" id="slide-38">
  <div>
    <h2>Polynomial regression</h2>
      
<div class='left' style='float:left;width:48%'>
<p>\[ \hat{weight}= 261.88- 7.35\times height +0.083 \times height^2 \]
Both regression coefficients are significant at the p &lt; 0.0001 level. The amount of variance accounted for has increased to 99.9 percent. The significance of the squared term (t = 13.89, p &lt; .001) suggests that inclusion of the quadratic term improves the model fit.</p>

</div>    
<div class='right' style='float:right;width:50%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height + I(height^2), data = women)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.50941 -0.29611 -0.00941  0.28615  0.59706 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 261.87818   25.19677  10.393 2.36e-07 ***
## height       -7.34832    0.77769  -9.449 6.58e-07 ***
## I(height^2)   0.08306    0.00598  13.891 9.32e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3841 on 12 degrees of freedom
## Multiple R-squared:  0.9995, Adjusted R-squared:  0.9994 
## F-statistic: 1.139e+04 on 2 and 12 DF,  p-value: &lt; 2.2e-16
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="non-linear">
  <div>
    <h2>Non-linear Data</h2>
    <p><img src="assets/fig/class02-chunk-37-1.png" alt="plot of chunk class02-chunk-37"></p>

  </div>
</section>
<section class="slide scode" id="slide-40">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code>## 
## Call:
## lm(formula = Y ~ X, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.00376 -0.41253 -0.00409  0.40664  0.85874 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.94111    0.09057   10.39   &lt;2e-16 ***
## X           -1.86189    0.15648  -11.90   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4585 on 99 degrees of freedom
## Multiple R-squared:  0.5885, Adjusted R-squared:  0.5843 
## F-statistic: 141.6 on 1 and 99 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide scode" id="slide-41">
  <div>
    <h2>Non-linear Data</h2>
    <p><img src="assets/fig/class02-chunk-39-1.png" alt="plot of chunk class02-chunk-39"></p>

  </div>
</section>
<section class="slide scode" id="slide-42">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.32331 -0.08538  0.00652  0.08320  0.20239 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.16341    0.04425  -3.693 0.000367 ***
## X            11.67844    0.38513  30.323  &lt; 2e-16 ***
## X2          -33.94179    0.89748 -37.819  &lt; 2e-16 ***
## X3           22.59349    0.58979  38.308  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1153 on 97 degrees of freedom
## Multiple R-squared:  0.9745, Adjusted R-squared:  0.9737 
## F-statistic:  1235 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-43">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
##     X10 + X11 + X12 + X13 + X14, data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.242662 -0.038179  0.002771  0.052484  0.210917 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -6.909e-02  8.413e-02  -0.821    0.414
## X            1.494e+01  1.056e+01   1.415    0.161
## X2          -2.609e+02  4.275e+02  -0.610    0.543
## X3           3.764e+03  7.863e+03   0.479    0.633
## X4          -3.203e+04  8.020e+04  -0.399    0.691
## X5           1.717e+05  5.050e+05   0.340    0.735
## X6          -6.225e+05  2.089e+06  -0.298    0.766
## X7           1.587e+06  5.881e+06   0.270    0.788
## X8          -2.889e+06  1.146e+07  -0.252    0.801
## X9           3.752e+06  1.544e+07   0.243    0.809
## X10         -3.398e+06  1.414e+07  -0.240    0.811
## X11          2.039e+06  8.384e+06   0.243    0.808
## X12         -7.276e+05  2.906e+06  -0.250    0.803
## X13          1.166e+05  4.467e+05   0.261    0.795
## X14                 NA         NA      NA       NA
## 
## Residual standard error: 0.09079 on 87 degrees of freedom
## Multiple R-squared:  0.9858, Adjusted R-squared:  0.9837 
## F-statistic: 465.2 on 13 and 87 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide compact sscode-nowrap" id="slide-44">
  <div>
    <h2>Non-linear Data</h2>
      
<div class='left' style='float:left;width:30%'>
<ul>
<li>new feature correlated with the old columns</li>
<li>the linear regression breaks down and can&#39;t find coefficients for all of the columns separately</li>
</ul>

</div>    
<div class='right' style='float:right;width:70%'>
  <pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
##     X10 + X11 + X12 + X13 + X14, data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.242662 -0.038179  0.002771  0.052484  0.210917 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -6.909e-02  8.413e-02  -0.821    0.414
## X            1.494e+01  1.056e+01   1.415    0.161
## X2          -2.609e+02  4.275e+02  -0.610    0.543
## X3           3.764e+03  7.863e+03   0.479    0.633
## X4          -3.203e+04  8.020e+04  -0.399    0.691
## X5           1.717e+05  5.050e+05   0.340    0.735
## X6          -6.225e+05  2.089e+06  -0.298    0.766
## X7           1.587e+06  5.881e+06   0.270    0.788
## X8          -2.889e+06  1.146e+07  -0.252    0.801
## X9           3.752e+06  1.544e+07   0.243    0.809
## X10         -3.398e+06  1.414e+07  -0.240    0.811
## X11          2.039e+06  8.384e+06   0.243    0.808
## X12         -7.276e+05  2.906e+06  -0.250    0.803
## X13          1.166e+05  4.467e+05   0.261    0.795
## X14                 NA         NA      NA       NA
## 
## Residual standard error: 0.09079 on 87 degrees of freedom
## Multiple R-squared:  0.9858, Adjusted R-squared:  0.9837 
## F-statistic: 465.2 on 13 and 87 DF,  p-value: &lt; 2.2e-16
</code></pre>

</div>
  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-45">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code>## 
## Call:
## lm(formula = Y ~ poly(X, degree = 14), data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.232557 -0.042933  0.002159  0.051021  0.209959 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             0.010167   0.009038   1.125   0.2638    
## poly(X, degree = 14)1  -5.455362   0.090827 -60.063  &lt; 2e-16 ***
## poly(X, degree = 14)2  -0.039389   0.090827  -0.434   0.6656    
## poly(X, degree = 14)3   4.418054   0.090827  48.642  &lt; 2e-16 ***
## poly(X, degree = 14)4  -0.047966   0.090827  -0.528   0.5988    
## poly(X, degree = 14)5  -0.706451   0.090827  -7.778 1.48e-11 ***
## poly(X, degree = 14)6  -0.204221   0.090827  -2.248   0.0271 *  
## poly(X, degree = 14)7  -0.051341   0.090827  -0.565   0.5734    
## poly(X, degree = 14)8  -0.031001   0.090827  -0.341   0.7337    
## poly(X, degree = 14)9   0.077232   0.090827   0.850   0.3975    
## poly(X, degree = 14)10  0.048088   0.090827   0.529   0.5979    
## poly(X, degree = 14)11  0.129990   0.090827   1.431   0.1560    
## poly(X, degree = 14)12  0.024726   0.090827   0.272   0.7861    
## poly(X, degree = 14)13  0.023706   0.090827   0.261   0.7947    
## poly(X, degree = 14)14  0.087906   0.090827   0.968   0.3358    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.09083 on 86 degrees of freedom
## Multiple R-squared:  0.986,  Adjusted R-squared:  0.9837 
## F-statistic: 431.7 on 14 and 86 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-46">
  <div>
    <h2>Plot Non-linear Data</h2>
    <p><img src="assets/fig/class02-chunk-44-1.png" alt="plot of chunk class02-chunk-44"></p>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-47">
  <div>
    <h2>Plot Non-linear Data (degree=3)</h2>
    <p><img src="assets/fig/class02-chunk-45-1.png" alt="plot of chunk class02-chunk-45"></p>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-48">
  <div>
    <h2>Plot Non-linear Data (degree=5)</h2>
    <p><img src="assets/fig/class02-chunk-46-1.png" alt="plot of chunk class02-chunk-46"></p>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-49">
  <div>
    <h2>Plot Non-linear Data (degree=25)</h2>
    <p><img src="assets/fig/class02-chunk-47-1.png" alt="plot of chunk class02-chunk-47"></p>

<p>model become too complex (too many parameters) and exaggerate minor fluctuations (noise) in the data</p>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="cross-validation">
  <div>
    <h2>Cross-validation</h2>
      
<div class='left' style='float:left;width:70%'>

</div>    
<div class='right' style='float:right;width:30%'>
  <p><img src="assets/fig/class02-chunk-49-1.png" alt="plot of chunk class02-chunk-49"></p>

</div>
  </div>
</section>
<section class="slide sscode-nowrap compact" id="regularization">
  <div>
    <h2>Regularization</h2>
    <pre><code>## 
## Call:  glmnet(x = poly(x, degree = 2), y = y) 
## 
##       Df   %Dev   Lambda
##  [1,]  0 0.0000 0.535300
##  [2,]  1 0.1009 0.487800
##  [3,]  1 0.1847 0.444500
##  [4,]  1 0.2543 0.405000
##  [5,]  1 0.3120 0.369000
##  [6,]  1 0.3599 0.336200
##  [7,]  1 0.3997 0.306300
##  [8,]  1 0.4328 0.279100
##  [9,]  1 0.4602 0.254300
## [10,]  1 0.4830 0.231700
## [11,]  1 0.5019 0.211200
## [12,]  1 0.5176 0.192400
## [13,]  1 0.5306 0.175300
## [14,]  1 0.5415 0.159700
## [15,]  1 0.5504 0.145500
## [16,]  1 0.5579 0.132600
## [17,]  1 0.5641 0.120800
## [18,]  1 0.5692 0.110100
## [19,]  1 0.5735 0.100300
## [20,]  1 0.5770 0.091400
## [21,]  1 0.5800 0.083280
## [22,]  1 0.5824 0.075880
## [23,]  1 0.5845 0.069140
## [24,]  1 0.5861 0.063000
## [25,]  1 0.5875 0.057400
## [26,]  1 0.5887 0.052300
## [27,]  1 0.5897 0.047660
## [28,]  1 0.5905 0.043420
## [29,]  1 0.5911 0.039570
## [30,]  1 0.5917 0.036050
## [31,]  1 0.5921 0.032850
## [32,]  1 0.5925 0.029930
## [33,]  1 0.5928 0.027270
## [34,]  1 0.5931 0.024850
## [35,]  1 0.5933 0.022640
## [36,]  1 0.5935 0.020630
## [37,]  1 0.5936 0.018800
## [38,]  1 0.5938 0.017130
## [39,]  1 0.5939 0.015610
## [40,]  1 0.5940 0.014220
## [41,]  2 0.5940 0.012960
## [42,]  2 0.5942 0.011810
## [43,]  2 0.5943 0.010760
## [44,]  2 0.5943 0.009801
## [45,]  2 0.5944 0.008930
## [46,]  2 0.5945 0.008137
## [47,]  2 0.5945 0.007414
## [48,]  2 0.5946 0.006755
## [49,]  2 0.5946 0.006155
## [50,]  2 0.5946 0.005608
## [51,]  2 0.5946 0.005110
## [52,]  2 0.5947 0.004656
## [53,]  2 0.5947 0.004243
## [54,]  2 0.5947 0.003866
## [55,]  2 0.5947 0.003522
## [56,]  2 0.5947 0.003209
## [57,]  2 0.5947 0.002924
## [58,]  2 0.5947 0.002664
## [59,]  2 0.5947 0.002428
</code></pre>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-52">
  <div>
    <h2>Regularization</h2>
      
<div class='left' style='float:left;width:70%'>

</div>    
<div class='right' style='float:right;width:30%'>
  <p><img src="assets/fig/class02-chunk-52-1.png" alt="plot of chunk class02-chunk-52"></p>

</div>
  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-53">
  <div>
    <h2>Regularization</h2>
    <pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        1
## (Intercept) -0.001051649
## 1           -4.969723232
## 2            .          
## 3            3.914406028
## 4            .          
## 5           -0.028623213
## 6            .          
## 7            .          
## 8            .          
## 9            .          
## 10           .
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="local-poly">
  <div>
    <h2>Local Polynomial Regression</h2>
    <pre><code>##     NOx  C     E
## 1 3.741 12 0.907
## 2 2.295 12 0.761
## 3 1.498 12 1.108
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-55">
  <div>
    <h2>Local Polynomial Regression</h2>
    <p><img src="assets/fig/class02-chunk-55-1.png" alt="plot of chunk class02-chunk-55"></p>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-56">
  <div>
    <h2>Local Polynomial Regression</h2>
    <p><img src="assets/fig/class02-chunk-56-1.png" alt="plot of chunk class02-chunk-56"></p>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-57">
  <div>
    <h2>Local Polynomial Regression</h2>
    <pre><code>##           [,1]     [,2]     [,3]      [,4]
## [1,] -3.220084 18.81266 16.42649 0.1183932
## [2,] -3.249601 17.61614 15.43623 0.1154507
## [3,] -3.319650 16.77004 14.75204 0.1151542
## [4,] -3.336464 15.44404 13.88921 0.1115457
## [5,] -3.373011 14.52391 13.11543 0.1099609
## [6,] -3.408908 13.96789 12.63493 0.1094681
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-58">
  <div>
    <h2>Local Polynomial Regression</h2>
    <p><img src="assets/fig/class02-chunk-58-1.png" alt="plot of chunk class02-chunk-58"></p>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-59">
  <div>
    <h2>Local Polynomial Regression</h2>
    <pre><code>## Call:
## locfit(formula = NOx ~ lp(E, nn = 0.3), data = ethanol)
## 
## Number of observations:          88 
## Family:  Gaussian 
## Fitted Degrees of freedom:       10.517 
## Residual scale:                  0.306
</code></pre>

<p><img src="assets/fig/class02-chunk-59-1.png" alt="plot of chunk class02-chunk-59"></p>

  </div>
</section>
<section class="slide scode-nowrap compact" id="lasso">
  <div>
    <h2>LASSO</h2>
    <pre><code>##       lcavol age      lbph       lcp gleason       lpsa
## 1 -0.5798185  50 -1.386294 -1.386294       6 -0.4307829
## 2 -0.9942523  58 -1.386294 -1.386294       6 -0.1625189
## 3 -0.5108256  74 -1.386294 -1.386294       7 -0.1625189
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-61">
  <div>
    <h2>LASSO</h2>
    <pre><code>## 
## Call:
## lm(formula = lcavol ~ ., data = prostate)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.88964 -0.52719 -0.07263  0.57834  1.98728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.49371    0.94261  -1.585   0.1165    
## age          0.01902    0.01063   1.789   0.0769 .  
## lbph        -0.08918    0.05376  -1.659   0.1006    
## lcp          0.29727    0.06762   4.396 2.98e-05 ***
## gleason      0.05240    0.11965   0.438   0.6625    
## lpsa         0.53955    0.07648   7.054 3.30e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7015 on 91 degrees of freedom
## Multiple R-squared:  0.6642, Adjusted R-squared:  0.6457 
## F-statistic:    36 on 5 and 91 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-62">
  <div>
    <h2>LASSO</h2>
    <pre><code>## LASSO sequence
## Computing X&#39;X .....
## LARS Step 1 :     Variable 5     added
## LARS Step 2 :     Variable 3     added
## LARS Step 3 :     Variable 1     added
## LARS Step 4 :     Variable 4     added
## LARS Step 5 :     Variable 2     added
## Computing residuals, RSS etc .....
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-63">
  <div>
    <h2>LASSO</h2>
    <p><img src="assets/fig/class02-chunk-63-1.png" alt="plot of chunk class02-chunk-63"></p>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-64">
  <div>
    <h2>LASSO</h2>
    <pre><code>## 
## Call:
## lars(x = x, y = prostate$lcavol, trace = TRUE)
## R-squared: 0.664 
## Sequence of LASSO moves:
##      lpsa lcp age gleason lbph
## Var     5   3   1       4    2
## Step    1   2   3       4    5
</code></pre>

<pre><code>##              age         lbph        lcp    gleason      lpsa
## [1,] 0.000000000  0.000000000 0.06519506 0.00000000 0.2128290
## [2,] 0.000000000  0.000000000 0.18564339 0.00000000 0.3587292
## [3,] 0.005369985 -0.001402051 0.28821232 0.01136331 0.4827810
## [4,] 0.019023772 -0.089182565 0.29727207 0.05239529 0.5395488
</code></pre>

  </div>
</section>
<section class="slide scode-nowrap compact" id="slide-65">
  <div>
    <h2>LASSO</h2>
    <p><img src="assets/fig/class02-chunk-65-1.png" alt="plot of chunk class02-chunk-65"></p>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-66">
  <div>
    <h2>LASSO</h2>
    <pre><code>## [1] 1.021938
</code></pre>

<pre><code>## [1] 0.6723226
</code></pre>

<pre><code>## [1] 0.5410033
</code></pre>

<pre><code>## [1] 0.5352386
</code></pre>

  </div>
</section>
<section class="slide sscode-nowrap compact" id="slide-67">
  <div>
    <h2>LASSO</h2>
    <p><img src="assets/fig/class02-chunk-67-1.png" alt="plot of chunk class02-chunk-67"></p>

  </div>
</section>
  <div class="progress">
    <div></div>
  </div>
    <footer class = 'foot'>
      <a href="index.html"><img src = 'assets/img/arrow_up_circle.png' style="width:30px;height:30px;"></a></img>
      <a href="#toc"><img src = 'assets/img/circle-info.png' style="width:32px;height:32px;"></a></img>
    </footer>    
	<script src="libraries/frameworks/shower/shower.js"></script>
	<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
	    tex2jax: {
	      inlineMath: [['$','$'], ['\\(','\\)']],
	      processEscapes: true
	    }
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script> -->
	<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
	<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<!-- DONE LOADING HIGHLIGHTER JS FILES -->
	 
		<!-- Copyright © 2010–2012 Vadim Makeev — pepelsbey.net -->
	<!-- Photos by John Carey — fiftyfootshadows.net -->
</body>
</html>