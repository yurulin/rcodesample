<!DOCTYPE HTML>
<html lang="en-US">
<head>
	<title>Class02</title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=1274, user-scalable=no">
	<meta name="description" content="Class02">
	<meta name="author" content="">
	<meta name="generator" content="slidify" />
	<!-- LOAD STYLE SHEETS -->
	<link rel="stylesheet" href="libraries/frameworks/shower/themes/ribbon/styles/screen.css">
	<link rel="stylesheet" media="print"
	  href="libraries/frameworks/shower/themes/ribbon/styles/print.css">
	<link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css">  <link rel="stylesheet" href = "assets/css/mystyle.css">
<link rel="stylesheet" href = "assets/css/ribbons.css">

	<!--
		To apply styles to the certain slides
		use slide ID to get needed elements
		-->
	<style>
		#Cover h2 {
      margin:65px 0 0;
			color:#FFF;
			text-align:center;
			font-size:70px;
			}
		#FitToWidth h2,
		#FitToHeight h2 {
			color:#FFF;
			text-align:center;
			}
	</style> 
</head>
<body class="list">
  <header class="caption">
  	<h1>Class02</h1>
	</header>
  <section class="slide " id="toc">
  <div>
    <h2>Class02</h2>
    <ul>
<li><a href="#set-up">Set up</a></li>
<li><a href="#simple-regression">Simple Regression</a></li>
<li><a href="#multiple-linear-regression">Multiple linear regression</a></li>
<li><a href="#variable-selection">Variable selection</a></li>
<li><a href="#model-performance">Model performance</a></li>
<li><a href="#polynomial-regression">Polynomial regression</a></li>
<li>Non-linear Data</li>
<li>Cross-validation</li>
<li>Regularization</li>
<li>Local Polynomial Regression</li>
<li>LASSO</li>
</ul>

  </div>
</section>
<section class="slide modal" id="set-up">
  <div>
    <h2>Install R packages</h2>
    <pre><code class="r">## this tutorial uses example data from the &#39;nutshell&#39; package
install.packages(&#39;nutshell&#39;) 
</code></pre>

  </div>
</section>
<section class="slide modal" id="simple-regression">
  <div>
    <h2>Simple Regression</h2>
    <p>The examples are taken from <a href="http://www.manning.com/kabacoff/">R in Action</a>
\[\hat{y}=\alpha +\beta x\]</p>

<pre><code class="r">## we&#39;ll use an exmaple dataset &#39;women&#39; in the &#39;car&#39; package
library(car) ## load the package
women
</code></pre>

<pre><code>##    height weight
## 1      58    115
## 2      59    117
## 3      60    120
## 4      61    123
## 5      62    126
## 6      63    129
## 7      64    132
## 8      65    135
## 9      66    139
## 10     67    142
## 11     68    146
## 12     69    150
## 13     70    154
## 14     71    159
## 15     72    164
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-4">
  <div>
    <h2>Simple Regression</h2>
    <pre><code class="r">## data summary
summary(women)
</code></pre>

<pre><code>##      height         weight     
##  Min.   :58.0   Min.   :115.0  
##  1st Qu.:61.5   1st Qu.:124.5  
##  Median :65.0   Median :135.0  
##  Mean   :65.0   Mean   :136.7  
##  3rd Qu.:68.5   3rd Qu.:148.0  
##  Max.   :72.0   Max.   :164.0
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-5">
  <div>
    <h2>Simple Regression</h2>
    <pre><code class="r">## plot the data
plot(women$height, women$weight)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-4-1.png" alt="plot of chunk unnamed-chunk-4"></p>

  </div>
</section>
<section class="slide modal" id="slide-6">
  <div>
    <h2>Simple Regression</h2>
    <pre><code class="r">library(ggplot2) ## load the plotting package
theme_set(theme_bw()) ## set default theme with a white background
ggplot(data=women, aes(x=height,y=weight)) + geom_point() 
</code></pre>

<p><img src="assets/fig/unnamed-chunk-5-1.png" alt="plot of chunk unnamed-chunk-5"></p>

  </div>
</section>
<section class="slide modal" id="slide-7">
  <div>
    <h2>Simple Regression</h2>
    <pre><code class="r">## plot with regression line
ggplot(women, aes(x = height, y = weight)) + geom_point() + 
  geom_smooth(method=lm, # add linear regression line
              se=FALSE) # (by default includes 95% confidence region)  
</code></pre>

<p><img src="assets/fig/unnamed-chunk-6-1.png" alt="plot of chunk unnamed-chunk-6"></p>

  </div>
</section>
<section class="slide scode" id="slide-8">
  <div>
    <h2>Simple Regression</h2>
    <pre><code class="r">## perform linear regression
fit = lm(weight ~ height, data=women)
summary(fit)
</code></pre>

<pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-9">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<p>\[ \hat{weight}=-87.52+3.45\times height \]</p>

<ul>
<li>There&#39;s an expected increase of 3.45 pounds of weight for every 1 inch increase in height. </li>
<li>The intercept is merely an adjustment constant.<br></li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-10">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>Is the model statistically significant?</strong></p>

<ul>
<li>Check the <strong>F statistic</strong> at the bottom of the summary.</li>
<li>The F statistic tells you whether the model is insignificant or significant. Big p-value indicates a high likelihood of insignificance.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-11">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>Are the coefficients significant?</strong></p>

<ul>
<li>Check the coefficient&#39;s <strong>t statistics</strong> and <strong>p-values</strong> in the summary, or check their confidence intervals.</li>
<li>If a variable&#39;s coefficient is zero then the variable is worthless; it adds nothing to the model. </li>
<li>The p-value is a probability that the coefficient is not significant. Big is bad because it indicates a high likelihood of insignificance.</li>
<li>The regression coefficient (3.45) is significantly different from zero (p &lt; 0.001).</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-12">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Is the model useful?</strong></p>

<ul>
<li>Check the <strong>R-squared</strong> near the bottom of the summary.</li>
<li>R-squared is a measure of the model&#39;s quality -- the fraction of the variance of y that is explained by the regression model. Bigger is better.</li>
<li>The multiple R-squared (0.991) indicates that the model accounts for 99.1 percent of the variance in weights. The multiple R-squared is also the squared correlation between the actual and predicted value.</li>
<li>The adjusted value accounts for the number of variables in your model and so is a more realistic assessment of its effectiveness.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-13">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>Does the model fit the data well?</strong></p>

<ul>
<li>Plot the residuals and check the regression diagnostics.</li>
<li>The residual standard error (1.53 lbs.) can be thought of as the average error in predicting weight from height using this model. </li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="slide-14">
  <div>
    <h2>Plot the residuals</h2>
    <pre><code class="r">## plot the residuals and check if the residuals appear to be approximately normal
plot(density(resid(fit)))
</code></pre>

<p><img src="assets/fig/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"></p>

  </div>
</section>
<section class="slide compact scode" id="slide-15">
  <div>
    <h2>Understand the summary</h2>
      
<div class='left' style='float:left;width:40%'>
<p><strong>Does the data satisfy the assumptions behind linear regression?</strong></p>

<ul>
<li>Check whether the diagnostics confirm that a linear model is reasonable for your data.</li>
<li>If the residuals have a normal distribution, then the first quartile (1Q) and third quartile (3Q) should have about the same magnitude.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="slide-16">
  <div>
    <h2>Plot the residuals</h2>
    <pre><code class="r">qqnorm(resid(fit)) # a quantile normal plot 
qqline(resid(fit))
</code></pre>

<p><img src="assets/fig/unnamed-chunk-15-1.png" alt="plot of chunk unnamed-chunk-15"></p>

  </div>
</section>
<section class="slide modal" id="slide-17">
  <div>
    <h2>Check statistical assumptions</h2>
    <pre><code class="r">## plot regression diagnostics
par(mfrow=c(2,2))
plot(fit)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-16-1.png" alt="plot of chunk unnamed-chunk-16"></p>

  </div>
</section>
<section class="slide compact scode" id="slide-18">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Normality</strong></p>

<ul>
<li>If the dependent variable is normally distributed for a fixed set of predictor values, then the residual values should be normally distributed with a mean of 0. </li>
<li>The <strong>Normal Q-Q plot</strong> (upper right) is a probability plot of the standardized residuals against the values that would be expected under normality. If you&#39;ve met the normality assumption, the points on this graph should fall on the straight 45-degree line. If they don&#39;t, you&#39;ve clearly violated the normality assumption.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/unnamed-chunk-17-1.png" alt="plot of chunk unnamed-chunk-17"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-19">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Independence</strong></p>

<ul>
<li>You have to use your understanding of how the data were collected. There&#39;s no a priori reason to believe that one woman&#39;s weight influences another woman&#39;s weight. If you found out that the data were sampled from families, you may have to adjust your assumption of independence.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/unnamed-chunk-18-1.png" alt="plot of chunk unnamed-chunk-18"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-20">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Linearity</strong></p>

<ul>
<li>If the dependent variable is linearly related to the independent variables, there should be no systematic relationship between the residuals and the predicted (that is, fitted) values. </li>
<li>In other words, the model should capture all the systematic variance present in the data, leaving nothing but random noise. </li>
<li>In the <strong>Residuals vs Fitted</strong> graph (upper left), you see clear evidence of a curved relationship, which suggests that you may want to add a quadratic term to the regression.

<ul>
<li>Homoscedasticity</li>
</ul></li>
<li>If you&#39;ve met the constant variance assumption, the points in the <strong>Scale-Location</strong> graph (bottom left) should be a random band around a horizontal line. You seem to meet this assumption.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/unnamed-chunk-19-1.png" alt="plot of chunk unnamed-chunk-19"></p>

</div>
  </div>
</section>
<section class="slide compact scode" id="slide-21">
  <div>
    <h2>Check statistical assumptions</h2>
      
<div class='left' style='float:left;width:50%'>
<p><strong>Homoscedasticity</strong></p>

<ul>
<li>If you&#39;ve met the constant variance assumption, the points in the <strong>Scale-Location</strong> graph (bottom left) should be a random band around a horizontal line. You seem to meet this assumption.</li>
</ul>

</div>    
<div class='right' style='float:right;width:50%'>
  <p><img src="assets/fig/unnamed-chunk-20-1.png" alt="plot of chunk unnamed-chunk-20"></p>

</div>
  </div>
</section>
<section class="slide modal" id="multiple-linear-regression">
  <div>
    <h2>Multiple linear regression</h2>
    <pre><code class="r">## we&#39;ll use the state.x77 dataset in the base package
states = as.data.frame(state.x77[,c(&quot;Murder&quot;, &quot;Population&quot;,
                                     &quot;Illiteracy&quot;, &quot;Income&quot;, &quot;Frost&quot;)])
head(states)
</code></pre>

<pre><code>##            Murder Population Illiteracy Income Frost
## Alabama      15.1       3615        2.1   3624    20
## Alaska       11.3        365        1.5   6315   152
## Arizona       7.8       2212        1.8   4530    15
## Arkansas     10.1       2110        1.9   3378    65
## California   10.3      21198        1.1   5114    20
## Colorado      6.8       2541        0.7   4884   166
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-23">
  <div>
    <h2>Examining data</h2>
    <pre><code class="r">cor(states)
</code></pre>

<pre><code>##                Murder Population Illiteracy     Income      Frost
## Murder      1.0000000  0.3436428  0.7029752 -0.2300776 -0.5388834
## Population  0.3436428  1.0000000  0.1076224  0.2082276 -0.3321525
## Illiteracy  0.7029752  0.1076224  1.0000000 -0.4370752 -0.6719470
## Income     -0.2300776  0.2082276 -0.4370752  1.0000000  0.2262822
## Frost      -0.5388834 -0.3321525 -0.6719470  0.2262822  1.0000000
</code></pre>

  </div>
</section>
<section class="slide modal" id="slide-24">
  <div>
    <h2>Examining data</h2>
    <pre><code class="r">## examining bivariate relationships using &#39;scatterplotMatrix&#39; in the &#39;car&#39; package
scatterplotMatrix(states, spread=FALSE, lty.smooth=2,
                  main=&quot;Scatter Plot Matrix&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-23-1.png" alt="plot of chunk unnamed-chunk-23"></p>

  </div>
</section>
<section class="slide modal" id="slide-25">
  <div>
    <h2>Multiple linear regression</h2>
    <pre><code class="r">fit = lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
summary(fit)
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-26">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<ul>
<li>In multiple regression, the coefficients indicate the increase in the dependent variable for a unit change in a predictor variable, <em>holding all other predictor variables constant</em>. </li>
<li>For example, the regression coefficient for Illiteracy is 4.14, suggesting that an increase of 1 percent in illiteracy is associated with a 4.14 percent increase in the murder rate, controlling for population, income, and temperature. </li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

</div>
  </div>
</section>
<section class="slide compact scode-nowrap" id="slide-27">
  <div>
    <h2>What does the result mean?</h2>
      
<div class='left' style='float:left;width:40%'>
<ul>
<li>The coefficient is significantly different from zero at the p &lt; .0001 level. The coefficient for Frost isn&#39;t significantly different from zero (p = 0.954) suggesting that Frost and Murder aren&#39;t linearly related when controlling for the other predictor variables. </li>
<li>Taken together, the predictor variables account for 57 percent of the variance in murder rates across states.</li>
</ul>

</div>    
<div class='right' style='float:right;width:60%'>
  <pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
##     data = states)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7960 -1.6495 -0.0811  1.4815  7.6210 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
## Population  2.237e-04  9.052e-05   2.471   0.0173 *  
## Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
## Income      6.442e-05  6.837e-04   0.094   0.9253    
## Frost       5.813e-04  1.005e-02   0.058   0.9541    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.535 on 45 degrees of freedom
## Multiple R-squared:  0.567,  Adjusted R-squared:  0.5285 
## F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
</code></pre>

</div>
  </div>
</section>
<section class="slide compact" id="slide-28">
  <div>
    <h2>Variable selection</h2>
    <p><strong>Stepwise selection</strong>: variables are added to or deleted from a model one at a time.</p>

<ul>
<li>forward: add predictor variables to the model one at a time, stopping when the addition of variables would no longer improve the model</li>
<li>backward: start with a model that includes all predictor variables, and then delete them one at a time until removing variables would degrade the quality of the model</li>
</ul>

  </div>
</section>
<section class="slide scode" id="slide-29">
  <div>
    <h2>Variable selection</h2>
    <pre><code class="r">## backward stepwise selection
library(MASS)
fit1 = lm(Murder ~ Population + Illiteracy + Income + Frost,
          data=states)
stepAIC(fit, direction=&quot;backward&quot;)
</code></pre>

<pre><code>## Start:  AIC=97.75
## Murder ~ Population + Illiteracy + Income + Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Frost       1     0.021 289.19  95.753
## - Income      1     0.057 289.22  95.759
## &lt;none&gt;                    289.17  97.749
## - Population  1    39.238 328.41 102.111
## - Illiteracy  1   144.264 433.43 115.986
## 
## Step:  AIC=95.75
## Murder ~ Population + Illiteracy + Income
## 
##              Df Sum of Sq    RSS     AIC
## - Income      1     0.057 289.25  93.763
## &lt;none&gt;                    289.19  95.753
## - Population  1    43.658 332.85 100.783
## - Illiteracy  1   236.196 525.38 123.605
## 
## Step:  AIC=93.76
## Murder ~ Population + Illiteracy
## 
##              Df Sum of Sq    RSS     AIC
## &lt;none&gt;                    289.25  93.763
## - Population  1    48.517 337.76  99.516
## - Illiteracy  1   299.646 588.89 127.311
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy, data = states)
## 
## Coefficients:
## (Intercept)   Population   Illiteracy  
##   1.6515497    0.0002242    4.0807366
</code></pre>

  </div>
</section>
<section class="slide sscode" id="slide-30">
  <div>
    <h2>Variable selection</h2>
    <pre><code>## Start:  AIC=97.75
## Murder ~ Population + Illiteracy + Income + Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Frost       1     0.021 289.19  95.753
## - Income      1     0.057 289.22  95.759
## &lt;none&gt;                    289.17  97.749
## - Population  1    39.238 328.41 102.111
## - Illiteracy  1   144.264 433.43 115.986
## 
## Step:  AIC=95.75
## Murder ~ Population + Illiteracy + Income
## 
##              Df Sum of Sq    RSS     AIC
## - Income      1     0.057 289.25  93.763
## &lt;none&gt;                    289.19  95.753
## - Population  1    43.658 332.85 100.783
## - Illiteracy  1   236.196 525.38 123.605
## 
## Step:  AIC=93.76
## Murder ~ Population + Illiteracy
## 
##              Df Sum of Sq    RSS     AIC
## &lt;none&gt;                    289.25  93.763
## - Population  1    48.517 337.76  99.516
## - Illiteracy  1   299.646 588.89 127.311
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Murder ~ Population + Illiteracy, data = states)
## 
## Coefficients:
## (Intercept)   Population   Illiteracy  
##   1.6515497    0.0002242    4.0807366
</code></pre>

  </div>
</section>
<section class="slide compact" id="slide-31">
  <div>
    <h2>Variable selection</h2>
    <p>You start with all four predictors in the model. For each step, the AIC column provides the model AIC resulting from the deletion of the variable listed in that row.
Models with smaller AIC values (indicating adequate fit with fewer parameters) are preferred.
Although stepwise selection may find a good model, there&#39;s no guarantee that it will find the best model because not every possible model is evaluated. (Alternative: all subsets method. See library <code>leaps</code>.)</p>

  </div>
</section>
<section class="slide compact scode" id="slide-32">
  <div>
    <h2>Model performance</h2>
    <pre><code class="r">## compute R-squared for the woman data
y = women$weight; x = women$height
fit = lm(y~x)
summary(fit)
</code></pre>

<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## x             3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-33">
  <div>
    <h2>Model performance</h2>
    <pre><code class="r">## compute R-squared for the woman data
mean.mse = mean((rep(mean(y),length(y)) - y)^2)
model.mse = mean(residuals(fit)^2)
rmse = sqrt(model.mse)
rmse ## root mean square error
</code></pre>

<pre><code>## [1] 1.419703
</code></pre>

<pre><code class="r">r2 = 1 - (model.mse / mean.mse)
r2
</code></pre>

<pre><code>## [1] 0.9910098
</code></pre>

<pre><code class="r">cor(y, fit$fitted.values)^2
</code></pre>

<pre><code>## [1] 0.9910098
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-34">
  <div>
    <h2>Model performance</h2>
    <pre><code class="r">## leave-one-out cross validation
n = length(women$weight)
error = dim(n)
for (k in 1:n) {
  train1 = c(1:n)
  train = train1[train1!=k] ## pick elements that are different from k
  m2 = lm(weight ~ height, data=women[train ,])
  pred = predict(m2, newdat=women[-train ,])
  obs = women$weight[-train]
  error[k] = obs-pred
}
me=mean(error)
me ## mean error
</code></pre>

<pre><code>## [1] 0.1148222
</code></pre>

<pre><code class="r">rmse=sqrt(mean(error^2))
rmse ## root mean square error (out-of-sample)
</code></pre>

<pre><code>## [1] 1.743782
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-35">
  <div>
    <h2>Polynomial regression</h2>
    <pre><code class="r">fit2 = lm(weight ~ height + I(height^2), data=women)
summary(fit2)
</code></pre>

<pre><code>## 
## Call:
## lm(formula = weight ~ height + I(height^2), data = women)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.50941 -0.29611 -0.00941  0.28615  0.59706 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 261.87818   25.19677  10.393 2.36e-07 ***
## height       -7.34832    0.77769  -9.449 6.58e-07 ***
## I(height^2)   0.08306    0.00598  13.891 9.32e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3841 on 12 degrees of freedom
## Multiple R-squared:  0.9995, Adjusted R-squared:  0.9994 
## F-statistic: 1.139e+04 on 2 and 12 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide compact scode" id="slide-36">
  <div>
    <h2>Polynomial regression</h2>
    <pre><code class="r">plot(women$height,women$weight,
     xlab=&quot;Height (in inches)&quot;,
     ylab=&quot;Weight (in lbs)&quot;)
lines(women$height,fitted(fit2))
</code></pre>

<p><img src="assets/fig/unnamed-chunk-33-1.png" alt="plot of chunk unnamed-chunk-33"></p>

  </div>
</section>
<section class="slide compact sscode" id="slide-37">
  <div>
    <h2>Polynomial regression</h2>
      
<div class='left' style='float:left;width:48%'>
<p>\[ \hat{weight}= 261.88- 7.35\times height +0.083 \times height^2 \]
Both regression coefficients are significant at the p &lt; 0.0001 level. The amount of variance accounted for has increased to 99.9 percent. The significance of the squared term (t = 13.89, p &lt; .001) suggests that inclusion of the quadratic term improves the model fit.</p>

</div>    
<div class='right' style='float:right;width:50%'>
  <pre><code>## 
## Call:
## lm(formula = weight ~ height + I(height^2), data = women)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.50941 -0.29611 -0.00941  0.28615  0.59706 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 261.87818   25.19677  10.393 2.36e-07 ***
## height       -7.34832    0.77769  -9.449 6.58e-07 ***
## I(height^2)   0.08306    0.00598  13.891 9.32e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3841 on 12 degrees of freedom
## Multiple R-squared:  0.9995, Adjusted R-squared:  0.9994 
## F-statistic: 1.139e+04 on 2 and 12 DF,  p-value: &lt; 2.2e-16
</code></pre>

</div>
  </div>
</section>
<section class="slide modal" id="slide-38">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code class="r">## create testing data (sine wave)
set.seed(1)
x = seq(0, 1, by = 0.01)
y = sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
df = data.frame(X = x, Y = y)
ggplot(df, aes(x = X, y = Y)) + geom_point()
</code></pre>

<p><img src="assets/fig/unnamed-chunk-35-1.png" alt="plot of chunk unnamed-chunk-35"></p>

  </div>
</section>
<section class="slide scode" id="slide-39">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code class="r">## fit with linear regression
summary(lm(Y ~ X, data = df)) # explain 60% of the variance
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Y ~ X, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.00376 -0.41253 -0.00409  0.40664  0.85874 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.94111    0.09057   10.39   &lt;2e-16 ***
## X           -1.86189    0.15648  -11.90   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4585 on 99 degrees of freedom
## Multiple R-squared:  0.5885, Adjusted R-squared:  0.5843 
## F-statistic: 141.6 on 1 and 99 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide scode" id="slide-40">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code class="r">ggplot(data.frame(X = x, Y = y), aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = &#39;lm&#39;, se = FALSE)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-37-1.png" alt="plot of chunk unnamed-chunk-37"></p>

  </div>
</section>
<section class="slide scode" id="slide-41">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code class="r">## add new features
df &lt;- transform(df, X2 = X ^ 2)
df &lt;- transform(df, X3 = X ^ 3)
summary(lm(Y ~ X + X2 + X3, data = df)) # R2 from 60% to 97%, by adding two more inputs
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.32331 -0.08538  0.00652  0.08320  0.20239 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.16341    0.04425  -3.693 0.000367 ***
## X            11.67844    0.38513  30.323  &lt; 2e-16 ***
## X2          -33.94179    0.89748 -37.819  &lt; 2e-16 ***
## X3           22.59349    0.58979  38.308  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1153 on 97 degrees of freedom
## Multiple R-squared:  0.9745, Adjusted R-squared:  0.9737 
## F-statistic:  1235 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide scode" id="slide-42">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code class="r">## add more features
df &lt;- transform(df, X4 = X ^ 4)
df &lt;- transform(df, X5 = X ^ 5)
df &lt;- transform(df, X6 = X ^ 6)
df &lt;- transform(df, X7 = X ^ 7)
df &lt;- transform(df, X8 = X ^ 8)
df &lt;- transform(df, X9 = X ^ 9)
df &lt;- transform(df, X10 = X ^ 10)
df &lt;- transform(df, X11 = X ^ 11)
df &lt;- transform(df, X12 = X ^ 12)
df &lt;- transform(df, X13 = X ^ 13)
df &lt;- transform(df, X14 = X ^ 14)
df &lt;- transform(df, X15 = X ^ 15)
summary(lm(Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 , data = df))
</code></pre>

<pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
##     X10 + X11 + X12 + X13 + X14, data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.242662 -0.038179  0.002771  0.052484  0.210917 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -6.909e-02  8.413e-02  -0.821    0.414
## X            1.494e+01  1.056e+01   1.415    0.161
## X2          -2.609e+02  4.275e+02  -0.610    0.543
## X3           3.764e+03  7.863e+03   0.479    0.633
## X4          -3.203e+04  8.020e+04  -0.399    0.691
## X5           1.717e+05  5.050e+05   0.340    0.735
## X6          -6.225e+05  2.089e+06  -0.298    0.766
## X7           1.587e+06  5.881e+06   0.270    0.788
## X8          -2.889e+06  1.146e+07  -0.252    0.801
## X9           3.752e+06  1.544e+07   0.243    0.809
## X10         -3.398e+06  1.414e+07  -0.240    0.811
## X11          2.039e+06  8.384e+06   0.243    0.808
## X12         -7.276e+05  2.906e+06  -0.250    0.803
## X13          1.166e+05  4.467e+05   0.261    0.795
## X14                 NA         NA      NA       NA
## 
## Residual standard error: 0.09079 on 87 degrees of freedom
## Multiple R-squared:  0.9858, Adjusted R-squared:  0.9837 
## F-statistic: 465.2 on 13 and 87 DF,  p-value: &lt; 2.2e-16
</code></pre>

  </div>
</section>
<section class="slide sscode compact" id="slide-43">
  <div>
    <h2>Non-linear Data</h2>
    <pre><code>## 
## Call:
## lm(formula = Y ~ X + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
##     X10 + X11 + X12 + X13 + X14, data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.242662 -0.038179  0.002771  0.052484  0.210917 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -6.909e-02  8.413e-02  -0.821    0.414
## X            1.494e+01  1.056e+01   1.415    0.161
## X2          -2.609e+02  4.275e+02  -0.610    0.543
## X3           3.764e+03  7.863e+03   0.479    0.633
## X4          -3.203e+04  8.020e+04  -0.399    0.691
## X5           1.717e+05  5.050e+05   0.340    0.735
## X6          -6.225e+05  2.089e+06  -0.298    0.766
## X7           1.587e+06  5.881e+06   0.270    0.788
## X8          -2.889e+06  1.146e+07  -0.252    0.801
## X9           3.752e+06  1.544e+07   0.243    0.809
## X10         -3.398e+06  1.414e+07  -0.240    0.811
## X11          2.039e+06  8.384e+06   0.243    0.808
## X12         -7.276e+05  2.906e+06  -0.250    0.803
## X13          1.166e+05  4.467e+05   0.261    0.795
## X14                 NA         NA      NA       NA
## 
## Residual standard error: 0.09079 on 87 degrees of freedom
## Multiple R-squared:  0.9858, Adjusted R-squared:  0.9837 
## F-statistic: 465.2 on 13 and 87 DF,  p-value: &lt; 2.2e-16
</code></pre>

<ul>
<li>new feature correlated with the old columns</li>
<li>the linear regression breaks down and can&#39;t find coefficients for all of the columns separately</li>
</ul>

  </div>
</section>
  <div class="progress">
    <div></div>
  </div>
    <footer class = 'foot'>
      <a href="index.html"><img src = 'assets/img/arrow_up_circle.png' style="width:30px;height:30px;"></a></img>
      <a href="#toc"><img src = 'assets/img/circle-info.png' style="width:32px;height:32px;"></a></img>
    </footer>    
	<script src="libraries/frameworks/shower/shower.js"></script>
	<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
	    tex2jax: {
	      inlineMath: [['$','$'], ['\\(','\\)']],
	      processEscapes: true
	    }
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script> -->
	<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
	<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<!-- DONE LOADING HIGHLIGHTER JS FILES -->
	 
		<!-- Copyright © 2010–2012 Vadim Makeev — pepelsbey.net -->
	<!-- Photos by John Carey — fiftyfootshadows.net -->
</body>
</html>